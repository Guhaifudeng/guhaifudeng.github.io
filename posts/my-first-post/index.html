<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™ | My New Hugo Site</title>
<meta name="keywords" content="">
<meta name="description" content="åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™v1.0222 å£°æ˜:æœ¬æ–‡æ¡£å†…å®¹ä»…æ˜¯ä¸ªäººç†è§£ï¼Œä¸å¯¹æœ¯è¯­çš„ä¸“ä¸šæ€§è´Ÿè´£ï¼Œä»…ä¾›å‚è€ƒã€‚
1 DL NLPæŠ€æœ¯èŒƒå¼ç®€å² æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŠ€æœ¯èŒƒå¼å¯åˆ†ä¸ºä»¥ä¸‹å››ä¸ªé˜¶æ®µï¼š
ç¬¬ä¸€é˜¶æ®µï¼Œè¯/å­—å‘é‡&#43;ç‰¹å®šä»»åŠ¡ç½‘ç»œç»“æ„è®¾è®¡
ä¾§é‡ç‚¹ï¼šç‰¹å®šä»»åŠ¡çš„ç½‘ç»œè®¾è®¡&#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹: é˜…è¯»ç†è§£ä»»åŠ¡çš„BiDAFã€QANET;æ–‡æœ¬åŒ¹é…çš„DSSMã€ARC-I;æœºå™¨ç¿»è¯‘çš„nmtç­‰
ç¬¬äºŒé˜¶æ®µï¼ŒåŸºäºBERT&#43;ç‰¹å®šä»»åŠ¡è¾“å‡ºå±‚ç½‘ç»œç»“æ„è®¾è®¡
ä¾§é‡ç‚¹ï¼šç›®æ ‡å‡½æ•°çš„è®¾è®¡&#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹ï¼šBERTã€MacBertã€XLNETç­‰
å‡ºå‘ç‚¹ï¼š
è¯­ä¹‰ä»»åŠ¡çš„è§£å†³ï¼Œéœ€è¦å¤æ‚çš„ç½‘ç»œï¼› å¤æ‚çš„ç½‘ç»œï¼Œéœ€è¦æ›´é«˜çš„æ ‡æ³¨æ•°æ®æˆæœ¬ï¼› é‡‡ç”¨æœ‰ç›‘ç£çš„è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥é™ä½æ•°æ®æˆæœ¬å¹¶æå‡æ€§èƒ½ã€‚ ç¬¬ä¸‰é˜¶æ®µï¼ŒåŸºäºLLM(å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹)çš„é¢å‘ç›®æ ‡ä»»åŠ¡å½¢å¼promptå·¥ç¨‹
ä¾§é‡ç‚¹:promptå·¥ç¨‹&#43;å¤šä»»åŠ¡å­¦ä¹ &#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹:GODELã€UIEç­‰
å‡ºå‘ç‚¹ï¼š
ç›®æ ‡ä»»åŠ¡æ²¡æœ‰å¾ˆå¥½åœ°åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ LLMæ¨¡å‹å¾®è°ƒæˆæœ¬é«˜ ä»é€‚åº”ä¸‹æ¸¸ä»»åŠ¡è½¬å˜ä¸ºé€‚åº”è¯­è¨€æ¨¡å‹ï¼Œä»¥å®ç°å¤šä»»åŠ¡å­¦ä¹ çš„ä¸€èˆ¬åŒ–ï¼ˆåˆå:ç»Ÿä¸€å»ºæ¨¡ï¼‰ ç¬¬å››é˜¶æ®µï¼Œé¢å‘instructçš„LLMçš„ä»»åŠ¡å­¦ä¹ èƒ½åŠ›
ä¾§é‡ç‚¹:ä»»åŠ¡è¿ç§»&#43;å®‰å…¨æ€§&#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹ï¼šT0ã€FLANã€instructGPTã€SUP-NATINSTç­‰
å‡ºå‘ç‚¹ï¼š
è¯­è¨€æ¨¡å‹å…·å¤‡è·¨ä»»åŠ¡èƒ½åŠ›è¿ç§»çš„èƒ½åŠ› è·¨ä»»åŠ¡çš„èƒ½åŠ›ä¹ å¾—éœ€è¦æ›´å¤šçš„æ•°æ®ï¼ˆç›¸æ¯”äºé¢„è®­ç»ƒæ•°æ®è§„æ¨¡å¤ªå°äº†ï¼‰ æ•°æ®å¢å¼ºçš„ä¸¤ä¸ªæ€è·¯ï¼š åŸºäºinstructå­¦ä¹ çš„æ–¹å¼é‡æ„æ›´å¤šçš„ä»»åŠ¡ï¼Œå¦‚SUP-NATINST ç›´æ¥åŸºäºinstructè¿›è¡Œä»¿äººç±»å›å¤ï¼Œå¦‚instructGPT å¯¹AIç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§æå‡ºè¦æ±‚ 2 è¯­è¨€æ¨¡å‹ æ¦‚è¿°è®ºæ–‡
A Survey of Transformers [2021]
https://export.arxiv.org/pdf/2106.04554.pdf
Efficient Transformers: A Survey [2020]
http://export.arxiv.org/pdf/2009.06732v3.pdf
Pre-Trained Models: Past, Present and Future [2021]
https://export.arxiv.org/pdf/2106.07139.pdf
A Survey on Knowledge-Enhanced Pre-trained Language Models [2022]
https://export.arxiv.org/pdf/2212.13428v1.pdf
Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey [2021]">
<meta name="author" content="">
<link rel="canonical" href="http://example.org/posts/my-first-post/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://example.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://example.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://example.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://example.org/apple-touch-icon.png">
<link rel="mask-icon" href="http://example.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™" />
<meta property="og:description" content="åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™v1.0222 å£°æ˜:æœ¬æ–‡æ¡£å†…å®¹ä»…æ˜¯ä¸ªäººç†è§£ï¼Œä¸å¯¹æœ¯è¯­çš„ä¸“ä¸šæ€§è´Ÿè´£ï¼Œä»…ä¾›å‚è€ƒã€‚
1 DL NLPæŠ€æœ¯èŒƒå¼ç®€å² æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŠ€æœ¯èŒƒå¼å¯åˆ†ä¸ºä»¥ä¸‹å››ä¸ªé˜¶æ®µï¼š
ç¬¬ä¸€é˜¶æ®µï¼Œè¯/å­—å‘é‡&#43;ç‰¹å®šä»»åŠ¡ç½‘ç»œç»“æ„è®¾è®¡
ä¾§é‡ç‚¹ï¼šç‰¹å®šä»»åŠ¡çš„ç½‘ç»œè®¾è®¡&#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹: é˜…è¯»ç†è§£ä»»åŠ¡çš„BiDAFã€QANET;æ–‡æœ¬åŒ¹é…çš„DSSMã€ARC-I;æœºå™¨ç¿»è¯‘çš„nmtç­‰
ç¬¬äºŒé˜¶æ®µï¼ŒåŸºäºBERT&#43;ç‰¹å®šä»»åŠ¡è¾“å‡ºå±‚ç½‘ç»œç»“æ„è®¾è®¡
ä¾§é‡ç‚¹ï¼šç›®æ ‡å‡½æ•°çš„è®¾è®¡&#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹ï¼šBERTã€MacBertã€XLNETç­‰
å‡ºå‘ç‚¹ï¼š
è¯­ä¹‰ä»»åŠ¡çš„è§£å†³ï¼Œéœ€è¦å¤æ‚çš„ç½‘ç»œï¼› å¤æ‚çš„ç½‘ç»œï¼Œéœ€è¦æ›´é«˜çš„æ ‡æ³¨æ•°æ®æˆæœ¬ï¼› é‡‡ç”¨æœ‰ç›‘ç£çš„è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥é™ä½æ•°æ®æˆæœ¬å¹¶æå‡æ€§èƒ½ã€‚ ç¬¬ä¸‰é˜¶æ®µï¼ŒåŸºäºLLM(å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹)çš„é¢å‘ç›®æ ‡ä»»åŠ¡å½¢å¼promptå·¥ç¨‹
ä¾§é‡ç‚¹:promptå·¥ç¨‹&#43;å¤šä»»åŠ¡å­¦ä¹ &#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹:GODELã€UIEç­‰
å‡ºå‘ç‚¹ï¼š
ç›®æ ‡ä»»åŠ¡æ²¡æœ‰å¾ˆå¥½åœ°åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ LLMæ¨¡å‹å¾®è°ƒæˆæœ¬é«˜ ä»é€‚åº”ä¸‹æ¸¸ä»»åŠ¡è½¬å˜ä¸ºé€‚åº”è¯­è¨€æ¨¡å‹ï¼Œä»¥å®ç°å¤šä»»åŠ¡å­¦ä¹ çš„ä¸€èˆ¬åŒ–ï¼ˆåˆå:ç»Ÿä¸€å»ºæ¨¡ï¼‰ ç¬¬å››é˜¶æ®µï¼Œé¢å‘instructçš„LLMçš„ä»»åŠ¡å­¦ä¹ èƒ½åŠ›
ä¾§é‡ç‚¹:ä»»åŠ¡è¿ç§»&#43;å®‰å…¨æ€§&#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹ï¼šT0ã€FLANã€instructGPTã€SUP-NATINSTç­‰
å‡ºå‘ç‚¹ï¼š
è¯­è¨€æ¨¡å‹å…·å¤‡è·¨ä»»åŠ¡èƒ½åŠ›è¿ç§»çš„èƒ½åŠ› è·¨ä»»åŠ¡çš„èƒ½åŠ›ä¹ å¾—éœ€è¦æ›´å¤šçš„æ•°æ®ï¼ˆç›¸æ¯”äºé¢„è®­ç»ƒæ•°æ®è§„æ¨¡å¤ªå°äº†ï¼‰ æ•°æ®å¢å¼ºçš„ä¸¤ä¸ªæ€è·¯ï¼š åŸºäºinstructå­¦ä¹ çš„æ–¹å¼é‡æ„æ›´å¤šçš„ä»»åŠ¡ï¼Œå¦‚SUP-NATINST ç›´æ¥åŸºäºinstructè¿›è¡Œä»¿äººç±»å›å¤ï¼Œå¦‚instructGPT å¯¹AIç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§æå‡ºè¦æ±‚ 2 è¯­è¨€æ¨¡å‹ æ¦‚è¿°è®ºæ–‡
A Survey of Transformers [2021]
https://export.arxiv.org/pdf/2106.04554.pdf
Efficient Transformers: A Survey [2020]
http://export.arxiv.org/pdf/2009.06732v3.pdf
Pre-Trained Models: Past, Present and Future [2021]
https://export.arxiv.org/pdf/2106.07139.pdf
A Survey on Knowledge-Enhanced Pre-trained Language Models [2022]
https://export.arxiv.org/pdf/2212.13428v1.pdf
Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey [2021]" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/my-first-post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-07T11:49:51+08:00" />
<meta property="article:modified_time" content="2023-07-07T11:49:51+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™"/>
<meta name="twitter:description" content="åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™v1.0222 å£°æ˜:æœ¬æ–‡æ¡£å†…å®¹ä»…æ˜¯ä¸ªäººç†è§£ï¼Œä¸å¯¹æœ¯è¯­çš„ä¸“ä¸šæ€§è´Ÿè´£ï¼Œä»…ä¾›å‚è€ƒã€‚
1 DL NLPæŠ€æœ¯èŒƒå¼ç®€å² æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŠ€æœ¯èŒƒå¼å¯åˆ†ä¸ºä»¥ä¸‹å››ä¸ªé˜¶æ®µï¼š
ç¬¬ä¸€é˜¶æ®µï¼Œè¯/å­—å‘é‡&#43;ç‰¹å®šä»»åŠ¡ç½‘ç»œç»“æ„è®¾è®¡
ä¾§é‡ç‚¹ï¼šç‰¹å®šä»»åŠ¡çš„ç½‘ç»œè®¾è®¡&#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹: é˜…è¯»ç†è§£ä»»åŠ¡çš„BiDAFã€QANET;æ–‡æœ¬åŒ¹é…çš„DSSMã€ARC-I;æœºå™¨ç¿»è¯‘çš„nmtç­‰
ç¬¬äºŒé˜¶æ®µï¼ŒåŸºäºBERT&#43;ç‰¹å®šä»»åŠ¡è¾“å‡ºå±‚ç½‘ç»œç»“æ„è®¾è®¡
ä¾§é‡ç‚¹ï¼šç›®æ ‡å‡½æ•°çš„è®¾è®¡&#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹ï¼šBERTã€MacBertã€XLNETç­‰
å‡ºå‘ç‚¹ï¼š
è¯­ä¹‰ä»»åŠ¡çš„è§£å†³ï¼Œéœ€è¦å¤æ‚çš„ç½‘ç»œï¼› å¤æ‚çš„ç½‘ç»œï¼Œéœ€è¦æ›´é«˜çš„æ ‡æ³¨æ•°æ®æˆæœ¬ï¼› é‡‡ç”¨æœ‰ç›‘ç£çš„è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥é™ä½æ•°æ®æˆæœ¬å¹¶æå‡æ€§èƒ½ã€‚ ç¬¬ä¸‰é˜¶æ®µï¼ŒåŸºäºLLM(å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹)çš„é¢å‘ç›®æ ‡ä»»åŠ¡å½¢å¼promptå·¥ç¨‹
ä¾§é‡ç‚¹:promptå·¥ç¨‹&#43;å¤šä»»åŠ¡å­¦ä¹ &#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹:GODELã€UIEç­‰
å‡ºå‘ç‚¹ï¼š
ç›®æ ‡ä»»åŠ¡æ²¡æœ‰å¾ˆå¥½åœ°åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ LLMæ¨¡å‹å¾®è°ƒæˆæœ¬é«˜ ä»é€‚åº”ä¸‹æ¸¸ä»»åŠ¡è½¬å˜ä¸ºé€‚åº”è¯­è¨€æ¨¡å‹ï¼Œä»¥å®ç°å¤šä»»åŠ¡å­¦ä¹ çš„ä¸€èˆ¬åŒ–ï¼ˆåˆå:ç»Ÿä¸€å»ºæ¨¡ï¼‰ ç¬¬å››é˜¶æ®µï¼Œé¢å‘instructçš„LLMçš„ä»»åŠ¡å­¦ä¹ èƒ½åŠ›
ä¾§é‡ç‚¹:ä»»åŠ¡è¿ç§»&#43;å®‰å…¨æ€§&#43;æ•°æ®å·¥ç¨‹
ä»£è¡¨æ¨¡å‹ï¼šT0ã€FLANã€instructGPTã€SUP-NATINSTç­‰
å‡ºå‘ç‚¹ï¼š
è¯­è¨€æ¨¡å‹å…·å¤‡è·¨ä»»åŠ¡èƒ½åŠ›è¿ç§»çš„èƒ½åŠ› è·¨ä»»åŠ¡çš„èƒ½åŠ›ä¹ å¾—éœ€è¦æ›´å¤šçš„æ•°æ®ï¼ˆç›¸æ¯”äºé¢„è®­ç»ƒæ•°æ®è§„æ¨¡å¤ªå°äº†ï¼‰ æ•°æ®å¢å¼ºçš„ä¸¤ä¸ªæ€è·¯ï¼š åŸºäºinstructå­¦ä¹ çš„æ–¹å¼é‡æ„æ›´å¤šçš„ä»»åŠ¡ï¼Œå¦‚SUP-NATINST ç›´æ¥åŸºäºinstructè¿›è¡Œä»¿äººç±»å›å¤ï¼Œå¦‚instructGPT å¯¹AIç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§æå‡ºè¦æ±‚ 2 è¯­è¨€æ¨¡å‹ æ¦‚è¿°è®ºæ–‡
A Survey of Transformers [2021]
https://export.arxiv.org/pdf/2106.04554.pdf
Efficient Transformers: A Survey [2020]
http://export.arxiv.org/pdf/2009.06732v3.pdf
Pre-Trained Models: Past, Present and Future [2021]
https://export.arxiv.org/pdf/2106.07139.pdf
A Survey on Knowledge-Enhanced Pre-trained Language Models [2022]
https://export.arxiv.org/pdf/2212.13428v1.pdf
Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey [2021]"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://example.org/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™",
      "item": "http://example.org/posts/my-first-post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™",
  "name": "åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™",
  "description": "åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™v1.0222 å£°æ˜:æœ¬æ–‡æ¡£å†…å®¹ä»…æ˜¯ä¸ªäººç†è§£ï¼Œä¸å¯¹æœ¯è¯­çš„ä¸“ä¸šæ€§è´Ÿè´£ï¼Œä»…ä¾›å‚è€ƒã€‚\n1 DL NLPæŠ€æœ¯èŒƒå¼ç®€å² æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŠ€æœ¯èŒƒå¼å¯åˆ†ä¸ºä»¥ä¸‹å››ä¸ªé˜¶æ®µï¼š\nç¬¬ä¸€é˜¶æ®µï¼Œè¯/å­—å‘é‡+ç‰¹å®šä»»åŠ¡ç½‘ç»œç»“æ„è®¾è®¡\nä¾§é‡ç‚¹ï¼šç‰¹å®šä»»åŠ¡çš„ç½‘ç»œè®¾è®¡+æ•°æ®å·¥ç¨‹\nä»£è¡¨æ¨¡å‹: é˜…è¯»ç†è§£ä»»åŠ¡çš„BiDAFã€QANET;æ–‡æœ¬åŒ¹é…çš„DSSMã€ARC-I;æœºå™¨ç¿»è¯‘çš„nmtç­‰\nç¬¬äºŒé˜¶æ®µï¼ŒåŸºäºBERT+ç‰¹å®šä»»åŠ¡è¾“å‡ºå±‚ç½‘ç»œç»“æ„è®¾è®¡\nä¾§é‡ç‚¹ï¼šç›®æ ‡å‡½æ•°çš„è®¾è®¡+æ•°æ®å·¥ç¨‹\nä»£è¡¨æ¨¡å‹ï¼šBERTã€MacBertã€XLNETç­‰\nå‡ºå‘ç‚¹ï¼š\nè¯­ä¹‰ä»»åŠ¡çš„è§£å†³ï¼Œéœ€è¦å¤æ‚çš„ç½‘ç»œï¼› å¤æ‚çš„ç½‘ç»œï¼Œéœ€è¦æ›´é«˜çš„æ ‡æ³¨æ•°æ®æˆæœ¬ï¼› é‡‡ç”¨æœ‰ç›‘ç£çš„è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥é™ä½æ•°æ®æˆæœ¬å¹¶æå‡æ€§èƒ½ã€‚ ç¬¬ä¸‰é˜¶æ®µï¼ŒåŸºäºLLM(å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹)çš„é¢å‘ç›®æ ‡ä»»åŠ¡å½¢å¼promptå·¥ç¨‹\nä¾§é‡ç‚¹:promptå·¥ç¨‹+å¤šä»»åŠ¡å­¦ä¹ +æ•°æ®å·¥ç¨‹\nä»£è¡¨æ¨¡å‹:GODELã€UIEç­‰\nå‡ºå‘ç‚¹ï¼š\nç›®æ ‡ä»»åŠ¡æ²¡æœ‰å¾ˆå¥½åœ°åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ LLMæ¨¡å‹å¾®è°ƒæˆæœ¬é«˜ ä»é€‚åº”ä¸‹æ¸¸ä»»åŠ¡è½¬å˜ä¸ºé€‚åº”è¯­è¨€æ¨¡å‹ï¼Œä»¥å®ç°å¤šä»»åŠ¡å­¦ä¹ çš„ä¸€èˆ¬åŒ–ï¼ˆåˆå:ç»Ÿä¸€å»ºæ¨¡ï¼‰ ç¬¬å››é˜¶æ®µï¼Œé¢å‘instructçš„LLMçš„ä»»åŠ¡å­¦ä¹ èƒ½åŠ›\nä¾§é‡ç‚¹:ä»»åŠ¡è¿ç§»+å®‰å…¨æ€§+æ•°æ®å·¥ç¨‹\nä»£è¡¨æ¨¡å‹ï¼šT0ã€FLANã€instructGPTã€SUP-NATINSTç­‰\nå‡ºå‘ç‚¹ï¼š\nè¯­è¨€æ¨¡å‹å…·å¤‡è·¨ä»»åŠ¡èƒ½åŠ›è¿ç§»çš„èƒ½åŠ› è·¨ä»»åŠ¡çš„èƒ½åŠ›ä¹ å¾—éœ€è¦æ›´å¤šçš„æ•°æ®ï¼ˆç›¸æ¯”äºé¢„è®­ç»ƒæ•°æ®è§„æ¨¡å¤ªå°äº†ï¼‰ æ•°æ®å¢å¼ºçš„ä¸¤ä¸ªæ€è·¯ï¼š åŸºäºinstructå­¦ä¹ çš„æ–¹å¼é‡æ„æ›´å¤šçš„ä»»åŠ¡ï¼Œå¦‚SUP-NATINST ç›´æ¥åŸºäºinstructè¿›è¡Œä»¿äººç±»å›å¤ï¼Œå¦‚instructGPT å¯¹AIç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§æå‡ºè¦æ±‚ 2 è¯­è¨€æ¨¡å‹ æ¦‚è¿°è®ºæ–‡\nA Survey of Transformers [2021]\nhttps://export.arxiv.org/pdf/2106.04554.pdf\nEfficient Transformers: A Survey [2020]\nhttp://export.arxiv.org/pdf/2009.06732v3.pdf\nPre-Trained Models: Past, Present and Future [2021]\nhttps://export.arxiv.org/pdf/2106.07139.pdf\nA Survey on Knowledge-Enhanced Pre-trained Language Models [2022]\nhttps://export.arxiv.org/pdf/2212.13428v1.pdf\nRecent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey [2021]",
  "keywords": [
    
  ],
  "articleBody": "åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™v1.0222 å£°æ˜:æœ¬æ–‡æ¡£å†…å®¹ä»…æ˜¯ä¸ªäººç†è§£ï¼Œä¸å¯¹æœ¯è¯­çš„ä¸“ä¸šæ€§è´Ÿè´£ï¼Œä»…ä¾›å‚è€ƒã€‚\n1 DL NLPæŠ€æœ¯èŒƒå¼ç®€å² æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŠ€æœ¯èŒƒå¼å¯åˆ†ä¸ºä»¥ä¸‹å››ä¸ªé˜¶æ®µï¼š\nç¬¬ä¸€é˜¶æ®µï¼Œè¯/å­—å‘é‡+ç‰¹å®šä»»åŠ¡ç½‘ç»œç»“æ„è®¾è®¡\nä¾§é‡ç‚¹ï¼šç‰¹å®šä»»åŠ¡çš„ç½‘ç»œè®¾è®¡+æ•°æ®å·¥ç¨‹\nä»£è¡¨æ¨¡å‹: é˜…è¯»ç†è§£ä»»åŠ¡çš„BiDAFã€QANET;æ–‡æœ¬åŒ¹é…çš„DSSMã€ARC-I;æœºå™¨ç¿»è¯‘çš„nmtç­‰\nç¬¬äºŒé˜¶æ®µï¼ŒåŸºäºBERT+ç‰¹å®šä»»åŠ¡è¾“å‡ºå±‚ç½‘ç»œç»“æ„è®¾è®¡\nä¾§é‡ç‚¹ï¼šç›®æ ‡å‡½æ•°çš„è®¾è®¡+æ•°æ®å·¥ç¨‹\nä»£è¡¨æ¨¡å‹ï¼šBERTã€MacBertã€XLNETç­‰\nå‡ºå‘ç‚¹ï¼š\nè¯­ä¹‰ä»»åŠ¡çš„è§£å†³ï¼Œéœ€è¦å¤æ‚çš„ç½‘ç»œï¼› å¤æ‚çš„ç½‘ç»œï¼Œéœ€è¦æ›´é«˜çš„æ ‡æ³¨æ•°æ®æˆæœ¬ï¼› é‡‡ç”¨æœ‰ç›‘ç£çš„è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥é™ä½æ•°æ®æˆæœ¬å¹¶æå‡æ€§èƒ½ã€‚ ç¬¬ä¸‰é˜¶æ®µï¼ŒåŸºäºLLM(å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹)çš„é¢å‘ç›®æ ‡ä»»åŠ¡å½¢å¼promptå·¥ç¨‹\nä¾§é‡ç‚¹:promptå·¥ç¨‹+å¤šä»»åŠ¡å­¦ä¹ +æ•°æ®å·¥ç¨‹\nä»£è¡¨æ¨¡å‹:GODELã€UIEç­‰\nå‡ºå‘ç‚¹ï¼š\nç›®æ ‡ä»»åŠ¡æ²¡æœ‰å¾ˆå¥½åœ°åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ LLMæ¨¡å‹å¾®è°ƒæˆæœ¬é«˜ ä»é€‚åº”ä¸‹æ¸¸ä»»åŠ¡è½¬å˜ä¸ºé€‚åº”è¯­è¨€æ¨¡å‹ï¼Œä»¥å®ç°å¤šä»»åŠ¡å­¦ä¹ çš„ä¸€èˆ¬åŒ–ï¼ˆåˆå:ç»Ÿä¸€å»ºæ¨¡ï¼‰ ç¬¬å››é˜¶æ®µï¼Œé¢å‘instructçš„LLMçš„ä»»åŠ¡å­¦ä¹ èƒ½åŠ›\nä¾§é‡ç‚¹:ä»»åŠ¡è¿ç§»+å®‰å…¨æ€§+æ•°æ®å·¥ç¨‹\nä»£è¡¨æ¨¡å‹ï¼šT0ã€FLANã€instructGPTã€SUP-NATINSTç­‰\nå‡ºå‘ç‚¹ï¼š\nè¯­è¨€æ¨¡å‹å…·å¤‡è·¨ä»»åŠ¡èƒ½åŠ›è¿ç§»çš„èƒ½åŠ› è·¨ä»»åŠ¡çš„èƒ½åŠ›ä¹ å¾—éœ€è¦æ›´å¤šçš„æ•°æ®ï¼ˆç›¸æ¯”äºé¢„è®­ç»ƒæ•°æ®è§„æ¨¡å¤ªå°äº†ï¼‰ æ•°æ®å¢å¼ºçš„ä¸¤ä¸ªæ€è·¯ï¼š åŸºäºinstructå­¦ä¹ çš„æ–¹å¼é‡æ„æ›´å¤šçš„ä»»åŠ¡ï¼Œå¦‚SUP-NATINST ç›´æ¥åŸºäºinstructè¿›è¡Œä»¿äººç±»å›å¤ï¼Œå¦‚instructGPT å¯¹AIç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§æå‡ºè¦æ±‚ 2 è¯­è¨€æ¨¡å‹ æ¦‚è¿°è®ºæ–‡\nA Survey of Transformers [2021]\nhttps://export.arxiv.org/pdf/2106.04554.pdf\nEfficient Transformers: A Survey [2020]\nhttp://export.arxiv.org/pdf/2009.06732v3.pdf\nPre-Trained Models: Past, Present and Future [2021]\nhttps://export.arxiv.org/pdf/2106.07139.pdf\nA Survey on Knowledge-Enhanced Pre-trained Language Models [2022]\nhttps://export.arxiv.org/pdf/2212.13428v1.pdf\nRecent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey [2021]\nhttps://export.arxiv.org/pdf/2111.01243.pdf\nLARGE LANGUAGE MODELS ARE HUMAN-LEVELPROMPT ENGINEERS [2022]\nhttps://export.arxiv.org/pdf/2211.01910v1.pdf\nA Survey on Transformers in Reinforcement Learning [2023]\nhttps://export.arxiv.org/pdf/2301.03044v1.pdf\nTransformers in Vision: A Survey [2022]\nhttps://arxiv.org/abs/2101.01169v5\nTransformeræ¨¡å‹å¤§ç›˜ç‚¹2023ç‰ˆï½œå‰Quoraå·¥ç¨‹å‰¯æ€»è£ï¼ˆè¶…é•¿ï¼‰\nhttps://hub.baai.ac.cn/view/24175\nhttps://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/\nä»£è¡¨è®ºæ–‡\nLanguage Models are Few-Shot Learners\nhttps://arxiv.org/pdf/2005.14165.pdf\n**(å¾ˆé‡è¦)**T5:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nâ€‹\thttps://arxiv.org/abs/1910.10683\nâ€‹\thttps://github.com/google-research/text-to-text-transfer-transformer\nThe C4 dataset we created for unsupervised pre-training is available in TensorFlow Datasets, but it requires a significant amount of bandwidth for downloading the raw Common Crawl scrapes (~7 TB) and compute for its preparation (~335 CPU-days)\nExT5: Towards Extreme Multi-Task Scaling for Transfer Learning\nhttps://arxiv.org/abs/2111.10952\nå¼€æºæ¨¡å‹æˆ–ä»£ç \nChatYuan: å…ƒè¯­åŠŸèƒ½å‹å¯¹è¯å¤§æ¨¡å‹\nhttps://github.com/clue-ai/ChatYuan\nhttps://mp.weixin.qq.com/s/-axa6XcjGl_Koeq_OrDq8w\nhttps://github.com/clue-ai/PromptCLUE\nhttps://github.com/clue-ai/clueai-python\nChatYuan-v2\n20230324æ›´æ–°ï¼š ChatYuanå¼€æºæ¨¡å‹å¤§å‡çº§ï¼Œæ•ˆæœæ˜æ˜¾æå‡ ChatYuan-large-v2æ˜¯ChatYuanç³»åˆ—ä¸­ä»¥è½»é‡åŒ–å®ç°é«˜è´¨é‡æ•ˆæœçš„æ¨¡å‹ä¹‹ä¸€ï¼Œæ”¯æŒè¾“å…¥è¾“å‡ºæ€»é•¿åº¦æœ€é•¿4kï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§æ˜¾å¡ã€ PCç”šè‡³æ‰‹æœºä¸Šè¿›è¡Œæ¨ç†ï¼ˆINT4 æœ€ä½åªéœ€ 400M ï¼‰ã€‚ æ¬¢è¿ä½“éªŒä½¿ç”¨ 1. å¼€æºé¡¹ç›®ï¼šhttps://github.com/clue-ai/ChatYuan 2. å¼€æºæ¨¡å‹hfåœ°å€ï¼šhttps://huggingface.co/ClueAI/ChatYuan-large-v2 3. å¼€æºæ¨¡å‹msåœ°å€ï¼šhttps://modelscope.cn/models/ClueAI/ChatYuan-large-v2/summary 4. æ¨¡å‹hfä½“éªŒåœ°å€ï¼ˆæ¨èï¼‰ï¼šhttps://huggingface.co/spaces/ClueAI/ChatYuan-large-v2 5. æ¨¡å‹msä½“éªŒåœ°å€ï¼ˆæš‚æ—¶æœ‰é—®é¢˜ï¼‰ï¼šhttps://modelscope.cn/studios/ClueAI/ChatYuan-large-v2/summary GPTç›¸å…³ https://github.com/Morizeyao/GPT2-Chinese\nhttps://github.com/Guhaifudeng/gpt-2\nT5è‹±æ–‡ç‰ˆ T5ç¬¬ä¸€ç‰ˆæœ¬ä»…æ”¯æŒè‹±æ–‡ï¼Œåé€€å‡ºå¤šè¯­è¨€ç‰ˆæœ¬mT5â€“è§‚å¯Ÿè¯è¡¨\nhttps://huggingface.co/t5-large\nhttps://huggingface.co/google/mt5-large\nhttps://huggingface.co/google/flan-t5-large\nhttps://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md\nT5ä¸­æ–‡ç‰ˆæœ¬ T5ä¸­æ–‡æ›¿ä»£å“:\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nhttps://huggingface.co/IDEA-CCNL/Randeng-T5-784Mã€€é¡¹ç›®åœ°å€\nhttps://huggingface.co/TsinghuaAI/CPM-Generate\nhttps://huggingface.co/IDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese\nhttps://huggingface.co/IDEA-CCNL/Randeng-T5-784M-QA-Chinese\nhttps://github.com/IDEA-CCNL/Fengshenbang-LM/tree/main/fengshen/examples/qa_t5\nhttps://github.com/bojone/t5_in_bert4keras\nhttps://arxiv.org/abs/1912.08777 https://github.com/shuxinyin/T5-NLP\nBARTç›¸å…³\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nhttps://huggingface.co/fnlp/bart-large-chinese\nhttps://github.com/fastnlp/CPT\nhttps://github.com/facebookresearch/fairseq/tree/main/examples/bart\nï¼ˆé‡è¦ï¼‰çº¿æ€§è¯­è¨€æ¨¡å‹RWKV é‡‡ç”¨RNNå®ç°ï¼Œæ•ˆç‡å’Œæ€§èƒ½æ¥è¿‘äºçº¿æ€§Transformer https://github.com/BlinkDL/RWKV-LM (explanation, fine-tuning, training, etc.) RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling\nhttps://github.com/dreasysnail/RetGen\nhttps://pypi.org/project/rwkvstic/ Easy pip package (with 8bit \u0026 offload for low VRAM GPUs)\nhttps://github.com/harrisonvanderbyl/rwkv_chatbot Chatbot using rwkvstic\nhttps://github.com/gururise/rwkv_gradio RWKV Gradio\nhttps://github.com/mrsteyk/RWKV-LM-deepspeed Another training fork\nhttps://github.com/Blealtan/RWKV-LM-LoRA LoRA fine-tuning\nhttps://github.com/wozeparrot/tinyrwkv RWKV in tinygrad (nice simple DL framework)\nhuggingface/transformers#17230 RWKV HF package (WIP)\nhttps://github.com/ArEnSc/Production-RWKV RWKV HF package source\nhttps://github.com/nlpodyssey/verbaflow RWKV in Go\nhttps://github.com/nlpodyssey/rwkv RWKV in Go\nhttps://github.com/mrsteyk/rwkvk-rs RWKV in Rust\nhttps://github.com/imxcstar/CSharp-RWKV-V4 RWKV in C#\nhttps://github.com/resloved/RWKV-notebooks RWKV colab notebooks\nhttps://colab.research.google.com/github/harrisonvanderbyl/rwkvstic/blob/master/notebooks/chatbot.ipynb RWKV chatbot colab notebook\nhttps://github.com/Pathos14489/RWKVDistributedInference RWKV Distributed Inference\nhttps://github.com/AXKuhta/rwkv-onnx-dml RWKV ONNX\nhttps://github.com/josephrocca/rwkv-v4-web RWKV-v4 running in the browser (simple demo. greedy decode)\n3 è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§ Language Models as Knowledge Bases?\nhttps://github.com/facebookresearch/LAMA\nHow Can We Know What Language Models Know?\nhttps://github.com/jzbjyb/LPAQA\nMaking Pre-trained Language Models Better Few-shot Learners\nWhat Makes Good In-Context Examples for GPT-$3$?\nçŸ¥æ™“T5-Largeä¸èƒ½åšä»€ä¹ˆ\n(é‡è¦-ç”Ÿæˆæç¤º) LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS\nå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯äººç±»çº§åˆ«çš„æç¤ºå·¥ç¨‹å¸ˆ:è€ƒè™‘é‡‡ç”¨chatGPTæ‰©å±•æç¤º\nTextBox: A Unified, Modularized, and Extensible Framework for Text Generation https://github.com/RUCAIBox/TextBox\nRethinking the Role of Demonstrations: What Makes In-Context Learning Work?\nhttps://export.arxiv.org/pdf/2202.12837.pdf\n4 promptå­¦ä¹ èŒƒå¼ æ¦‚è¿°è®ºæ–‡\n(é‡è¦)Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing http://pretrain.nlpedia.ai/\nä»£è¡¨è®ºæ–‡\n(é‡è¦)The Power of Scale for Parameter-Efficient Prompt Tuning\nhttps://arxiv.org/abs/2104.08691 https://github.com/kipgparker/soft-prompt-tuning\nè§£è¯»:https://code84.com/745392.html\nåœ¨å…¨é‡æ•°æ®æƒ…å†µä¸‹ï¼Œä»…å¾®è°ƒ prompt ç›¸å…³çš„å‚æ•°ï¼Œèƒ½å¦åª²ç¾ç”šè‡³è¶…è¿‡ fine-tuning çš„è¡¨ç°ï¼Ÿ\nåœ¨å°‘é‡æ•°æ®æƒ…å†µä¸‹ï¼Œä»…å¾®è°ƒ prompt ç›¸å…³çš„å‚æ•°ï¼Œèƒ½å¦åª²ç¾ç”šè‡³è¶…è¿‡ fine-tuning çš„è¡¨ç°ï¼Ÿ\nå¦‚æœèƒ½åšåˆ°ä¸Šè¿°è¡¨ç°ï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„å°ºå¯¸æ˜¯å¦æœ‰å½±å“ï¼Ÿæ˜¯å¦ä¸€å®šéœ€è¦è¶…å¤§é¢„è®­ç»ƒæ¨¡å‹ï¼Ÿ\nç»“è®ºï¼š\nprompt tokensé€‰æ‹©20è¯å·¦å³ æ„å»ºå¥½promtæŒ‰ç…§è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡è®­ç»ƒ50Kï¼Œç”šè‡³100Kæ­¥ prompt tokençš„åˆå§‹åŒ–å¯¹ç»“æœå½±å“å¾ˆå¤§ ä»¥ä¸Šç­–ç•¥åœ¨1Bçº§åˆ«åŠä»¥ä¸‹æœ‰æ•ˆï¼Œè¶…è¿‡10Bï¼Œå½±å“å˜å° PPT: Pre-trained Prompt Tuning for Few-shot Learning\nhttps://arxiv.org/abs/2109.04332\nSPoT: Better Frozen Model Adaptation through Soft Prompt Transfer\nhttps://arxiv.org/abs/2110.07904\nZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization]\nhttps://arxiv.org/abs/2201.06910\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nhttps://arxiv.org/abs/1910.10683\nUnified Structure Generation for Universal Information Extraction\nhttps://arxiv.org/abs/2203.12277\nMultitask Prompted Training Enables Zero-Shot Task Generalization\nhttps://arxiv.org/abs/2110.08207\nLearning To Retrieve Prompts for In-Context Learning\nå¼€æºæ¨¡å‹åŠä»£ç \nOpenPrompt: An Open-source Framework for Prompt-learning\n(é‡è¦)Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation\nè‡ªåŠ¨ç”ŸæˆPrompt https://github.com/princeton-nlp/LM-BFF\nç”Ÿæˆå¼ä»»åŠ¡å°è¯•\nhttps://github.com/clue-ai/PromptCLUE\nä¸‰å¤§ç»Ÿä¸€ï¼šç»Ÿä¸€æ¨¡å‹æ¡†æ¶(text-to-text)ï¼Œç»Ÿä¸€ä»»åŠ¡å½¢å¼(prompt)ï¼Œç»Ÿä¸€åº”ç”¨æ–¹å¼(zero-shot/few-shot)ã€‚ (T0ï¼‰\nå¤§è§„æ¨¡é¢„è®­ç»ƒï¼šåœ¨t5-largeç‰ˆåŸºç¡€ä¸Šï¼Œä½¿ç”¨æ•°ç™¾Gä¸­æ–‡è¯­æ–™ï¼Œè®­ç»ƒäº†100ä¸‡æ­¥ï¼Œç´¯ç§¯è®­ç»ƒäº†1.5ä¸‡äº¿ä¸ªä¸­æ–‡å­—è¯çº§åˆ«token\nå¤§è§„æ¨¡ä»»åŠ¡æ•°æ®ï¼šä½¿ç”¨äº†16ç§ä»»åŠ¡ç±»å‹ï¼Œæ•°ç™¾ç§ä»»åŠ¡ï¼Œç´¯ç§¯äº¿çº§åˆ«ä»»åŠ¡æ•°æ®ã€‚\næ··åˆé¢„è®­ç»ƒï¼šä¸€æ–¹é¢å°†ä¸‹æ¸¸ä»»åŠ¡ä½œä¸ºé¢„è®­ç»ƒè¯­æ–™ï¼Œå¦ä¸€æ–¹é¢å°†ä¸‹æ¸¸ä»»åŠ¡å’Œé¢„è®­ç»ƒè¯­æ–™ä¸€èµ·è®­ç»ƒï¼Œå‡å°‘ä»»åŠ¡ç¾éš¾é—å¿˜ä»¥åŠç¼©çŸ­é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡çš„è·ç¦»ï¼Œæ›´å¥½çš„é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼ˆExT5ï¼‰\næ··åˆé‡‡æ ·ï¼šé’ˆå¯¹ä¼—å¤šæ•°æ®é‡å·®å¼‚æå¤§çš„ä»»åŠ¡ï¼Œé‡‡ç”¨åœ¨æ¯ä¸ªè®­ç»ƒbatchå†…å¯¹æ‰€æœ‰çš„ä»»åŠ¡è¿›è¡ŒæŒ‰ç…§æ¯”ä¾‹é‡‡æ ·ï¼Œæ ¹æ®ä»»åŠ¡çš„æ•°æ®é‡è¿›è¡Œå¹³æ»‘é‡‡æ ·ï¼Œå¹¶ä¸”åŒæ—¶é™åˆ¶ä»»åŠ¡æ•°æ®é‡é‡‡æ ·æ± çš„ä¸Šé™ã€‚ å¹³æ»‘é‡‡æ ·å¯ä»¥å‡å°‘ä»»åŠ¡è®­ç»ƒæœ‰åå±å®³ï¼Œåœ¨æ¯ä¸€batchå†…è®­ç»ƒå¯ä»¥å‡å°‘å¼‚è´¨ä»»åŠ¡ä¹‹é—´è®­ç»ƒè´Ÿè¿ç§»çš„æƒ…å†µ(T5)\nåˆ†é˜¶æ®µè®­ç»ƒï¼šä¸€æ–¹é¢æŒ‡åœ¨é¢„è®­ç»ƒåˆ†é˜¶æ®µï¼Œæ¶‰åŠè®­ç»ƒåºåˆ—é•¿åº¦çš„åˆ†é˜¶æ®µï¼ˆ128å’Œ512ï¼‰ï¼ŒåŠ å¿«é¢„è®­ç»ƒé€Ÿåº¦(Bert)ï¼›å¦ä¸€æ–¹é¢ï¼Œåœ¨ä¸‹æ¸¸è®­ç»ƒåˆ†é˜¶æ®µï¼Œ æ¶‰åŠå­¦ä¹ ç‡å’Œåºåˆ—é•¿åº¦çš„å˜åŒ–ä»¥åŠé€’å‡å¼å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®é‡é™åˆ¶ï¼Œæ›´å¥½çš„é€‚åº”ä¸‹æ¸¸çš„ä¸åŒä»»åŠ¡\nå¢åŠ è¯­è¨€æ¨¡å‹çš„è®­ç»ƒï¼šå‚è€ƒt5.1.1, é™¤äº†ä½¿ç”¨Span Corrputionæ„å»ºçš„æ–¹å¼è¿›è¡Œæ— ç›‘ç£è®­ç»ƒï¼ŒåŒæ—¶åœ¨ä½¿ç”¨prefix LMçš„æ–¹å¼è®­ç»ƒï¼Œå¢å¼ºç”Ÿæˆä»»åŠ¡çš„èƒ½åŠ›(LM adapted)\nå¢åŠ å¯¹æ¨¡å‹çš„encoderä»¥åŠdecoderçš„è®­ç»ƒï¼šæ ¹æ®ä¸‹æ¸¸ä»»åŠ¡æ•°æ®åˆ†åˆ«æ„å»ºData_text,Data_targeté¢„è®­ç»ƒæ•°æ®è¯­æ–™ï¼Œæ˜¯åŠ å…¥åˆ°é¢„è®­ç»ƒä¸­ï¼Œåˆ†åˆ«å¢å¼ºæ¨¡å‹çš„encoderç†è§£èƒ½åŠ›å’Œ decoderçš„ç”Ÿæˆèƒ½åŠ›ï¼ˆè§UIEï¼‰\né‡æ–°æ„å»ºæ¨¡å‹ä¸­æ–‡å­—å…¸ï¼šä½¿ç”¨sentencepieceä¸Šåœ¨åƒäº¿tokenä¸Šå­¦ä¹ å¹¶æ„å»ºæ¨¡å‹å­—å…¸ï¼Œæ›´åŠ ç¬¦åˆä¸­æ–‡è¯­è¨€ä¹ æƒ¯\nå…¶ä»–\næ‰©å±•è®ºæ–‡æ±‡æ€»\nåœ°å€:github.com/thunlp/PromptPapers NLPçš„â€œç¬¬å››èŒƒå¼â€ä¹‹Prompt Learningæ€»ç»“ï¼š44ç¯‡è®ºæ–‡é€ä¸€æ¢³ç†\nåœ°å€\nåŸºäºpromptçš„æ–°è®­ç»ƒèŒƒå¼\nç›¸æ¯”ä¹‹å‰æ¯ä¸ªä»»åŠ¡å®šä¹‰ä¸€å¥—å‚æ•°ï¼Œåœ¨è¾“å…¥åŠ ä¸Šç‰¹å®šçš„ä¿¡æ¯ï¼Œä¸éœ€è¦æ”¹å˜æ•´ä¸ªæ¨¡å‹çš„å‚æ•°ï¼Œä»è€Œæå‡æ•ˆç‡å’Œå­˜å‚¨ç©ºé—´ã€‚\nä¼ ç»Ÿ pretrain+fintune çš„è®­ç»ƒæ–¹å¼æ˜¯æœ‰ gap çš„ï¼Œéœ€è¦ä»å¤§è§„æ¨¡æ— ç›‘ç£æ•°æ®è®­ç»ƒè¿ç§»åˆ°ä¸‹æ¸¸ finetune çš„ä»»åŠ¡ï¼Œprompt-based çš„æ–¹å¼æ‰“ç ´äº†è¿™ä¸ªæ–¹å¼ã€‚\nPromptæ–¹æ³•ç»¼è¿°\nhttps://zhuanlan.zhihu.com/p/431788068\nä¸€æ–‡äº†è§£é¢„è®­ç»ƒæ¨¡å‹ Prompt å¾®è°ƒï¼ˆæ¯”è¾ƒè¯¦ç»†ï¼‰\nhttps://zhuanlan.zhihu.com/p/572970562\n5 æ–‡æœ¬ç”Ÿæˆ Evaluation of Text Generation: A Survey [2020]\nhttps://arxiv.org/pdf/2006.14799.pdf\nA Survey of Evaluation Metrics Used for NLG Systems [2020]\nhttp://export.arxiv.org/pdf/2008.12009v2.pdf\n6 ç”Ÿæˆå¼å¯¹è¯ å¼€æºæ¨¡å‹åŠä»£ç  T5åœ¨ä¼šè¯é¢†åŸŸçš„åº”ç”¨ https://huggingface.co/microsoft/GODEL-v1_1-large-seq2seq https://huggingface.co/openbmb/cpm-ant-10b/tree/main\nhttps://huggingface.co/TsinghuaAI/CPM-Generate\n7 åŸºäºinstructçš„å¤šä»»åŠ¡å­¦ä¹  ä»£è¡¨è®ºæ–‡\nCross-Task Generalization via Natural Language Crowdsourcing Instructions Cross-task generalization via natural language crowdsourcing instructions Super-NaturalInstructions:Generalization via Declarative Instructions on 1600+ Tasks\nhttps://github.com/allenai/natural-instructions\nhttps://instructions.apps.allenai.org/\nFLAN:FINETUNED LANGUAGE MODELS ARE ZERO-SHOTLEARNERS http://export.arxiv.org/pdf/2109.01652v5.pdf FLANï¼šå¾®è°ƒè¯­è¨€æ¨¡å‹æ˜¯Zero-Shotå­¦ä¹ å™¨ http://www.syrr.cn/news/7832.html è°·æ­ŒFLAN-T5ä½œè€…äº²è®²ï¼š5400äº¿å‚æ•°ï¼Œ1800ä¸ªä»»åŠ¡ï¼Œå¦‚ä½•å®ç°å¤§è¯­è¨€æ¨¡å‹â€œè‡ªæˆ‘æ”¹è¿›â€ åœ°å€\nchatGPTç›¸å…³\nï¼ˆé‡è¦ï¼‰Survey for In-context Learning\nhttps://arxiv.org/pdf/2301.00234v1.pdf\nWebGPT: Browser-assisted question-answering with human feedback\nhttps://arxiv.org/abs/2112.09332\nLessons Learned on Language Model Safety and Misuse\nhttps://openai.com/blog/language-model-safety-and-misuse/\nstructGPT:Scaling Laws for Reward Model Overoptimization\nhttps://arxiv.org/abs/2210.10760\nLearning to summarize with human feedback\nhttps://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html\n8 chatGPTçš„prompts chatGPTçš„è‹±æ–‡prompt https://github.com/f/awesome-chatgpt-prompts\nchatGPTçš„ä¸­æ–‡prompt\nhttps://github.com/PlexPt/awesome-chatgpt-prompts-zh\nchatGPTæŠ€èƒ½æ¢ç´¢\n9 é¢„è®­ç»ƒæ•°æ® The Pile: An 800GB Dataset of Diverse Text for Language Modeling https://arxiv.org/abs/2101.00027\nChatGPT æ•°æ®é›†ä¹‹è°œ åœ°å€\n10 è®­ç»ƒæˆæœ¬åˆ†æ æ ·ä¾‹1 å†»ç»“éƒ¨åˆ†å‚æ•°é¢„è®­ç»ƒbig model https://huggingface.co/IDEA-CCNL/Randeng-T5-784M\næˆ‘ä»¬åŸºäºmT5-largeï¼Œè®­ç»ƒäº†å®ƒçš„ä¸­æ–‡ç‰ˆã€‚ä¸ºäº†åŠ é€Ÿè®­ç»ƒï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨T5åˆ†è¯å™¨(sentence piece)ä¸­çš„ä¸­è‹±æ–‡å¯¹åº”çš„è¯è¡¨ï¼Œå¹¶ä¸”ä½¿ç”¨äº†è¯­æ–™åº“è‡ªé€‚åº”é¢„è®­ç»ƒ(Corpus-Adaptive Pre-Training, CAPT)æŠ€æœ¯åœ¨æ‚Ÿé“è¯­æ–™åº“(180Gç‰ˆæœ¬)ç»§ç»­é¢„è®­ç»ƒã€‚é¢„è®­ç»ƒç›®æ ‡ä¸ºç ´åspanã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒé˜¶æ®µä¸­ä½¿ç”¨äº†å°ç¥æ¡†æ¶å¤§æ¦‚èŠ±è´¹äº†16å¼ A100çº¦96å°æ—¶ã€‚\nä½¿ç”¨è¯„ä¼°ç½‘ç«™ï¼šhttps://www.autodl.com/home\nä¸Šè¿°è®­ç»ƒèŠ±è´¹ä»£ä»·(é‡‡ç”¨16å¼ A100è®­ç»ƒ180Gè¯­æ–™-ä»…ä»…è®­ç»ƒäº†è¯è¡¨çš„embeddingï¼Œè¯¦è§è®ºæ–‡)ï¼š7.98å…ƒ/h x 16 x 96=12257.28å…ƒ\næ ·ä¾‹2 å…¨éƒ¨å‚æ•°é¢„è®­ç»ƒ big model https://www.zhiu.cn/156272.html 11 å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶ DeepSpeed https://www.deepspeed.ai/ ColossalAI https://github.com/hpcaitech/ColossalAI 12 é’ˆå¯¹chatGPTçš„è®­ç»ƒè¿‡ç¨‹å¤ç° æ ·ä¾‹\ncolossal-ai-chatgpt Open source solution replicates ChatGPT training process! Ready to go with only 1.6GB GPU memory and gives you 7.73 times faster training!\nhttps://www.hpc-ai.tech/blog/colossal-ai-chatgpt\nåŸºäºæƒ…æ„Ÿåˆ†ç±»çš„ç±»chatGPTè®­ç»ƒ\nå®Œæ•´æºç åœ¨è¿™é‡Œï¼š\nhttps://github.com/HarderThenHarder/transformers_tasks/tree/main/RLHF\né‡è¦æ€è·¯\nå®Œå…¨ä»é›¶å®ç°chatGPT-Andrej Karpathy Bç«™æ¬è¿ https://space.bilibili.com/3129054/channel/collectiondetail?sid=874339 è¯¾ç¨‹ä¸»é¡µ:https://github.com/karpathy/nn-zero-to-hero é‡ç‚¹é¡¹ç›®::https://github.com/karpathy/nanoGPT\nå¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›åˆ†æä¸åº”ç”¨ã€€é‚±é”¡é¹å¤æ—¦å¤§å­¦ https://www.bilibili.com/video/BV1Tx4y1w78p\n13 å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»‹ç» Illustrating Reinforcement Learning from Human Feedback (RLHF) https://huggingface.co/blog/rlhf 14 chatGPTæŠ€æœ¯æ€è€ƒ ChatGPT èƒŒåçš„â€œåŠŸè‡£â€â€”â€”RLHF æŠ€æœ¯è¯¦è§£ åœ°å€ è§£è¯» ChatGPT èƒŒåçš„æŠ€æœ¯é‡ç‚¹ï¼šRLHFã€IFTã€CoTã€çº¢è“å¯¹æŠ— åœ°å€\n(é‡è¦)What Makes a Dialog Agent Useful? https://huggingface.co/blog/dialog-agents\nçº¢è“å¯¹æŠ— (red-teaming) https://arxiv.org/abs/2209.07858\nå¼ æ ‹ï¼šChatGPT åˆ¶èƒœå…¬å¼ https://hub.baai.ac.cn/view/24166\n(é‡è¦)How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources\nhttps://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1\n(é‡è¦)Chain of Thought Prompting Elicits Reasoning in Large Language Models\nhttps://arxiv.org/pdf/2201.11903v1.pdf\næ·±å…¥ç†è§£è¯­è¨€æ¨¡å‹çš„çªç°èƒ½åŠ›\nhttps://yaofu.notion.site/514f4e63918749398a1a8a4c660e0d5b\n15 chatGPTå½±å“ è”æƒ³CTOèŠ®å‹‡ï¼šAIå¤§æ¨¡å‹ä¸ºæ™ºèƒ½åŒ–å˜é©å¸¦æ¥çš„æœºé‡å’ŒæŒ‘æˆ˜\nhttps://hub.baai.ac.cn/view/24171\nReid Hoffmanï½œç›¸ä¿¡è‡ªå·±ï¼Œå…¶å®ä½ æ¯”æƒ³è±¡ä¸­æ›´æœ‰å‡†å¤‡\nä¸­é‡‘ | AIåå¹´å±•æœ›ï¼ˆäº”ï¼‰ï¼šä»ChatGPTåˆ°é€šç”¨æ™ºèƒ½ï¼Œæ–°é•¿å¾ä¸Šçš„æ–°å˜åŒ– åœ°å€\nAIGCï¼šChatGPT(ä¸€ä¸ªé‡Œç¨‹ç¢‘å¼çš„å¯¹è¯èŠå¤©æœºå™¨äºº)çš„ç®€ä»‹(æ„ä¹‰/åŠŸèƒ½/æ ¸å¿ƒæŠ€æœ¯ç­‰)ã€ä½¿ç”¨æ–¹æ³•(ä¸ƒç±»ä»»åŠ¡)ã€æ¡ˆä¾‹åº”ç”¨(æé—®åŸºç¡€æ€§/äº‹å®æ€§/é€»è¾‘æ€§/åˆ›é€ æ€§/å¼€æ”¾æ€§çš„é—®é¢˜ä»¥åŠç¼–ç¨‹ç›¸å…³)ä¹‹è¯¦ç»†æ”»ç•¥ åœ°å€\näººå·¥æ™ºèƒ½è¡Œä¸šChatGPTä¸“é¢˜ç ”ç©¶ï¼šå¼€å¯AIæ–°çºªå…ƒ.pdf https://www.vzkoo.com/document/2023020328d4913255f87e4c853de0b8.html\nhttps://hub.baai.ac.cn/view/24173\n16 Attentionç›¸å…³ æµ‹è¯•ä¸¤ç§æ–° Attention æœºåˆ¶ï¼šgMLP å’Œ AFTï¼ˆç»“è®ºï¼šAFT æ•ˆæœå¥½ï¼‰ https://zhuanlan.zhihu.com/p/395005917?utm_id=0\nRWKV is inspired by Appleâ€™s AFT (https://arxiv.org/abs/2105.14103).\nMoreover itâ€™s using a number of my tricks, such as:\nSmallInitEmb: https://github.com/BlinkDL/SmallInitEmb (applicable to all transformers) which helps the embedding quality, and stabilizes Post-LN (which is what I am using). Token-shift: https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing (applicable to all transformers), especially helpful for char-level models. Head-QK: https://github.com/BlinkDL/RWKV-LM#the-head-qk-trick-learning-to-copy-and-avoid-tokens (applicable to all transformers). Note: itâ€™s helpful, but I disabled it in the Pile model to keep it 100% RNN. Extra R-gate in the FFN (applicable to all transformers). I am also using reluSquared from Primer. Better initilization: I init most of the matrices to ZERO (see RWKV_Init in https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v2-RNN/src/model.py). You can transfer some parameters from a small model to a large model (note: I sort \u0026 smooth them too), for faster and better convergence (see https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/). My CUDA kernel: https://github.com/BlinkDL/RWKV-CUDA to speedup training. 17 LMaaSæ€ç»´é“¾ è®ºæ–‡æ¦‚è¿°æˆ–é›†åˆ Towards Reasoning in Large Language Models: A Survey. https://arxiv.org/abs/2212.10403 A trend starts from â€œChain of Thought Prompting Elicits Reasoning in Large Language Modelsâ€. https://github.com/Timothyxxx/Chain-of-ThoughtsPapers ä»£è¡¨è®ºæ–‡ æ€ç»´é“¾æç¤º (Wei ç­‰, â€˜22): https://arxiv.org/abs/2201.11903 Letâ€™s think step by step: https://arxiv.org/abs/2205.11916 CoT å›¾è§£ç¤ºä¾‹ (Chung ç­‰, â€˜22): https://arxiv.org/abs/2210.11416 CoT å¾®è°ƒä¹Ÿæ˜¾ç¤ºå‡ºå¯¹æ— å®³æ€§éå¸¸æœ‰æ•ˆ (Bai ç­‰, â€˜22): https://www.anthropic.com/constitutional.pdf Large Language Models Are Reasoning Teachers. https://arxiv.org/abs/2212.10071 å¼€æ”¾æ¨¡å‹åŠä»£ç  Automatic Chain of Thought Prompting in Large Language Models https://github.com/amazon-science/auto-cot 18 LMaaSå®‰å…¨æ€§ Unnatural Instructions (Honovich ç­‰, â€˜22): https://arxiv.org/abs/2212.09689 Super-natural instructions (Wang ç­‰, â€˜22): https://arxiv.org/abs/2204.07705 Self-Instruct (Wang ç­‰, â€˜22): https://arxiv.org/abs/2212.10560 T0 (Sanh ç­‰, â€˜22): https://arxiv.org/abs/2110.08207 Natural instructions æ•°æ®é›† (Mishra ç­‰, â€˜22): https://arxiv.org/abs/2104.08773 FLAN LM (Wei ç­‰, â€˜22): https://arxiv.org/abs/2109.01652 OPT-IML (Iyer ç­‰, â€˜22): https://arxiv.org/abs/2212.12017 19 å¯¹è¯æœºå™¨äººäº§å“ Meta çš„ BlenderBot: https://arxiv.org/abs/2208.03188 Google çš„ LaMDA: https://arxiv.org/abs/2201.08239 DeepMind çš„ Sparrow: https://arxiv.org/abs/2209.14375 Anthropic çš„ Assistant: https://arxiv.org/abs/2204.05862 20 å¯¹è¯æ•°æ®é›† ä¸‰ä¸ªæ•°æ®é›†ï¼š ï¼ˆï¼‘ï¼‰é—²èŠã€€ï¼ˆï¼’ï¼‰åŸºäºæ–‡æ¡£çš„é—®é¢˜å›å¤ã€€Dureader Dureader*checklist*é˜…è¯»ç†è§£ç»†ç²’åº¦è¯„ä¼°æ•°æ®é›†â€“ä¸å¯èƒ½çš„ç›´æ¥ç»™æ–‡æ¡£\nâ€‹\tDuReader*robust*é˜…è¯»ç†è§£é²æ£’æ€§æ•°æ®é›†\nï¼ˆï¼“ï¼‰å¤šè½®ä¼šè¯è¯­æ–™\nâ€‹\tDuLeMonä¸­æ–‡é•¿æ—¶è®°å¿†å¯¹è¯æ•°æ®é›†\nâ€‹\tDiamanteä¸­æ–‡å¼€æ”¾åŸŸé—²èŠæ•°æ®é›†\nâ€‹\tDuSincæœåŠ¡ä¿¡æ¯å¢å¼ºå¯¹è¯æ•°æ®é›†\nâ€‹\tCrossWOZ\nâ€‹\thttps://github.com/thu-coai/CrossWOZ\n21 å…¶ä»–äººæ€»ç»“chatGPT å…³äºChatGPTçš„é¢„è®­ç»ƒå’Œè°ƒä¼˜æ–¹æ³•çš„å¿…è¯»è®ºæ–‡ã€ç›¸å…³åšå®¢å’ŒAPIå·¥å…·ã€‚ https://chatgpt.pro/ https://github.com/shizhediao/ChatGPTPapers å·²å£°æ˜ç±»chatGPTå†…æµ‹é˜¶æ®µçš„æœºæ„æˆ–å…¬å¸ äº¬ä¸œ ç™¾åº¦ é˜¿é‡Œ è…¾è®¯ chatYuan å¤æ—¦ 22 å¤šæŠ€èƒ½å¯¹è¯ ä»»åŠ¡å‹å¯¹è¯\nA Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation\nEstimating Soft Labels for Out-of-Domain Intent Detection\nè¡¨æ ¼å‹å¯¹è¯\nSTAR: SQL Guided Pre-Training for Context-dependent Text-to-SQL Parsing\nTowards Generalizable and Robust Text-to-SQL Parsing\næ–‡æ¡£å‹å¯¹è¯\nTowards Generalized Open Information Extraction\nDoc2Bot: Accessing Heterogeneous Documents via Conversational Bots\nå¤šæ¨¡æ€å¯¹è¯\nå¯¹è¯ç³»ç»Ÿçš„ç»ˆèº«å­¦ä¹ \nPrompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue\nSemi-Supervised Lifelong Language Learning\n23 çŸ¥è¯†å‹å¯¹è¯ åŠ¨æ€çŸ¥è¯†å¯¹è¯\nSINC: Service Information Augmented Open-Domain Conversation åˆå:Link the World: Improving Open-domain Conversation with Dynamic Spatiotemporal-aware Knowledge https://arxiv.org/pdf/2206.14000v2.pdf æ˜¾å¼ç›®æ ‡å¯¹è¯\nDuConv: Proactive Human-Machine Conversation with Explicit Conversation Goals https://arxiv.org/pdf/1906.05572v2.pdf çŸ¥è¯†å¯¹è¯\nKdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation https://arxiv.org/pdf/2004.04100v1.pdf\nKPT: Keyword-guided Pre-training for Grounded Dialog Generation\n24 å¯¹è¯è¡¨ç¤ºå­¦ä¹  dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings\nhttps://arxiv.org/abs/2210.15332v1\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial2vec\n25 å›½å†…å¤§å‚å¯¹è¯ç³»ç»Ÿå®éªŒå®¤ è¾¾æ‘©é™¢çš„SPACE SPACE-1: https://arxiv.org/abs/2111.14592 SPACE-2: https://arxiv.org/abs/2209.06638 SPACE-3: https://arxiv.org/abs/2209.06664 ç›¸å…³ä»£ç ï¼šhttps://github.com/AlibabaResearch/DAMO-ConvAI ç™¾åº¦çš„Knover PLATO-1\nPLATO-2\nPLATO-XL\nPLATO-KAG(PLATO-K)\nMarch 2022: We are opening PLATO-KAG, an unsupervised learning approach for end-to-end knowledge-grounded conversation modeling. February 2022: We are opening our TOD-DA dataset, models and code in DSTC10-Track2. December 2021: We are opening the dialogue generation model of PLATO-XL, with up to 11 billion parameters. October 2021: We are opening AG-DST, an amendable generation for dialogue state tracking. February 2021: We are opening our implementation (Team 19) in DSTC9-Track1. July 2020: We are opening PLATO-2, a large-scale generative model with latent space for open-domain dialogue systems. [1] TOD-DA: Towards Boosting the Robustness of Task-oriented Dialogue Modeling on Spoken Conversations\n[2] Learning to Select External Knowledge with Multi-Scale Negative Sampling\n[3] PLATO-KAG: Unsupervised Knowledge-Grounded Conversation via Joint Modeling\n[4] PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation\n[5] PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable\nè…¾è®¯AI lab Dialogue Research https://ai.tencent.com/ailab/nlp/dialogue/\nRetrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework\nhttps://aclanthology.org/D19-1195.pdf\nhttps://github.com/jcyk/seqgen\n26 (å·¥ç¨‹åŒ–)TransformeråŠ é€Ÿ Efficient Transformers: A Survey\nhttp://export.arxiv.org/pdf/2009.06732v3.pdf\nLarge Transformer Model Inference Optimization https://lilianweng.github.io/posts/2023-01-10-inference-optimization/\nThe Transformer Family Version 2.0 https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/\n(ç•™å­˜)ChatGPT is not all you need. A State of the Art Review of large Generative AI models\nå¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹\n(ç•™å­˜)COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics\nçº¦æŸæ¥æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„è¯­ä¹‰æˆ–æ ·å¼\n(ç•™å­˜) Symmetry Teleportation for Accelerated Optimization æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå‚æ•°ç©ºé—´å¯¹ç§°æ€§çš„ä¼˜åŒ–æ–¹æ³•ï¼ˆsymmetry teleportationï¼‰ï¼Œè¿™æ˜¯åœ¨åœ¨å‚æ•°ç©ºé—´ä¸Šä¿æŒæŸå¤±ä¸å˜çš„ä¸€ç»„åŠ¨ä½œï¼Œå®ƒå…è®¸å‚æ•°ç§»åŠ¨å¾ˆå¤§çš„è·ç¦»ï¼Œä»¥æé«˜åç»­æ­¥éª¤çš„æ”¶æ•›é€Ÿåº¦ã€ã€‚æœ¬æ–‡ç®—æ³•åˆ©ç”¨äº†é«˜é˜¶æ™¯è§‚å‡ ä½•ï¼Œä½†åœ¨å¤§å¤šæ•°æ­¥éª¤ä¸­åªä½¿ç”¨æ¢¯åº¦ä¿¡æ¯ï¼Œä»è€Œé¿å…äº†äºŒé˜¶æ–¹æ³•çš„è®¡ç®—æˆæœ¬ã€‚\n(ç•™å­˜)Deep Bidirectional Language-Knowledge Graph Pretraining çŸ¥è¯†å›¾è°±+è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒ https://export.arxiv.org/pdf/2210.09338v2.pdf\n(ç•™å­˜) Are Pre-trained Convolutions Better than Pre-trained Transformers? ä¸è¦å°†é¢„è®­ç»ƒçš„è¿›æ­¥ä¸æ¶æ„çš„è¿›æ­¥æ··ä¸ºä¸€è°ˆ\nEfficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better\n(é‡è¦-ç²¾è¯») FastSeq: Make Sequence Generation Faster The proposed optimization techniques include an attention cache optimization, an efficient algorithm for** detecting repeated n-grams**, and an **asynchronous generation pipeline **with parallel I/O. https://github.com/microsoft/fastseq\n(ç•™å­˜) Query-driven Segment Selection for Ranking Long Documents\n(é‡è¦ï¼‰ Understanding and Overcoming the Challenges of Efficient Transformer Quantization. https://github.com/qualcomm-ai-research/transformer-quantization.\nFinetuning Pretrained Transformers into RNNs\nA Primer on Pretrained Multilingual Language Models\n(ç•™å­˜) AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing\nEL-Attention: Memory Efficient Lossless Attention for Generation\nè‡ªå›å½’æ¨¡å‹è½¬éè‡ªå›å½’\nSlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection and Slot Filling\nIncorporating history and future into non-autoregressive machine translation\nï¼ˆç•™å­˜ï¼‰Non-Autoregressive Neural Machine Translation\nAn Effective Non-Autoregressive Model for Spoken Language Understanding\nï¼ˆé‡è¦ï¼‰A Study of Non-autoregressive Model for Sequence Generation\nA Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond\n(é‡è¦)Directed Acyclic Transformer for Non-Autoregressive Machine Translation\nhttps://arxiv.org/abs/2205.07459\nlatent-GLAT: Glancing at Latent Variables for Parallel Text Generation\nhttps://github.com/baoy-nlp/latent-glat\nhttps://lilianweng.github.io/posts/2023-01-10-inference-optimization/\n27 ä½æ˜¾å­˜è¿è¡Œ ChatRWKV v2 (wip) can split the model to gpu+cpu or gpu+gpu: https://github.com/BlinkDL/ChatRWKV/tree/main/v2 (Stream mode soon) è¿™ä¸ªå¯ä»¥æŠŠæ¨¡å‹åˆ†åˆ°åŒå¡ï¼Œæˆ–è€…ä¸€éƒ¨åˆ†GPUä¸€éƒ¨åˆ†CPUï¼ˆé€‚åˆåˆšå¥½æ”¾ä¸ä¸‹çš„æƒ…å†µï¼‰ ç¨åå†åŠ  STREAM æ¨¡å¼ï¼Œåœ¨å°æ˜¾å­˜GPUè·‘å¤§æ¨¡å‹ï¼ˆåŠ è½½å‡ å±‚æ¨¡å‹ï¼Œè·‘å‡ å±‚ï¼Œå†åŠ è½½å‡ å±‚ï¼Œè·‘å‡ å±‚ï¼‰ 28 é—®é¢˜é‡å†™ ï¼ˆé‡è¦) Question Rewriting for Conversational Question Answering (2021å¹´)\nhttps://dl.acm.org/doi/pdf/10.1145/3437963.3441748\nå¯¹è¯å¼é—®é¢˜å›ç­”çš„é—®é¢˜é‡å†™\nReinforced Question Rewriting for Conversational Question Answering\nOpen-Domain Question Answering Goes Conversational via Question Rewriting\nhttps://github.com/akaysh/DenseQrecc\nhttps://lrec2022.lrec-conf.org/en/ https://github.com/Orange-OpenSource/COQAR\n(é‡è¦)Improving Multi-turn Dialogue Modelling with Utterance ReWriter\nhttps://github.com/liu-nlper/dialogue-utterance-rewriter.git\n(é‡è¦-ç²¾è¯»)Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration (Pan et al., 2019)\nRestoration-200K datasets\nhttps://ai.tencent.com/ailab/nlp/dialogue/datasets/Restoration-200K.zip\n**(é‡è¦) Robust Dialogue Utterance Rewriting as Sequence Tagging **\né‡‡ç”¨æ ‡æ³¨â€“é€Ÿåº¦åŠ å¿«\nUtterance Rewriting with Contrastive Learning in Multi-turn Dialogue\n(é‡è¦)SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration.\nhttps://github.com/NetEase-GameAI/SARG\nNAGå³Non-Autoregressive Model\nå…¶ä»–\næ–‡çŒ®ç»¼è¿°ä¹‹è¯­å¥é‡å†™, çœ‹è¿™ä¸€ç¯‡å°±å¤Ÿäº†!\nâ€‹\thttps://zhuanlan.zhihu.com/p/405209386\n29 å¯¹è¯ç³»ç»Ÿsurvey The AI Doctor Is In: A Survey of Task-Oriented Dialogue Systems for Healthcare Applications\nï¼ˆé€‚åˆï¼‰Recent Neural Methods on Slot Filling and Intent Classification for Task-Oriented Dialogue Systems: A Survey.\nA Survey on Spoken Language Understanding: Recent Advances and New Frontiers\nA Transformer based Multi-task Model for Domain Classification, Intent Detection and Slot-Filling\nMulti-Task Pre-Training for Plug-and-Play(å³ç”¨å³æ’) Task-Oriented Dialogue System\nå³æ’å³ç”¨å‹é¢å‘ä»»åŠ¡å¯¹è¯ç³»ç»Ÿçš„å¤šä»»åŠ¡é¢„è®­ç»ƒâ€“ç¬¬ä¸€æ¬¡è§è¿™ä¸ªææ³•ï¼ˆè½¬å˜æˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤ä»»åŠ¡-é‡‡ç”¨T5å¤§æ¨¡å‹ï¼‰\nhttps://export.arxiv.org/pdf/2109.14739.pdf\nhttps://github.com/awslabs/pptod\nï¼ˆé€‚åˆï¼‰Conversational Question Answering: A Survey.\nA survey: Conversational Knowledge Base Question Answering\n30 ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿ A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation Estimating Soft Labels for Out-of-Domain Intent Detection\nA GuessWhat?! Game for Goal-Oriented Visual Dialog: A Survey\nUser Utterance Acquisition for Training Task-Oriented Bots: A Review of Challenges, Techniques and Opportunities\nEstimating Soft Labels for Out-of-Domain Intent Detection\n(é‡è¦ï¼ç²¾è¯»)Multi-Task Pre-Training for Plug-and-Play(å³ç”¨å³æ’) Task-Oriented Dialogue System\nhttps://export.arxiv.org/pdf/2109.14739.pdf\nhttps://github.com/awslabs/pptod\nå­¦ä¹ ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒ\nA Unified Multi-task Learning Framework for Multi-goal Conversational Recommender Systems\n31 åŸºäºä¸Šä¸‹æ–‡çš„å›å¤æ£€ç´¢ Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems\nhttps://aichatbot.feishu.cn/file/boxcni2xhINHrIfnIkoEIUTWyGg\nA Sequential Matching Framework for Multi-turn Response Selection in Retrieval-based Chatbots\nhttps://export.arxiv.org/pdf/1710.11344.pdf\nå¯¹è¯è¡¨ç¤ºå­¦ä¹ -ç”¨äºæ£€ç´¢\ndial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings\nReasoning With Neural Tensor Networks for Knowledge Base Completion\nhttps://papers.nips.cc/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf\nMulti-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots\nhttps://github.com/chunyuanY/Dialogue\n32 Codeç”Ÿæˆ Repository-Level Prompt Generation for Large Language Models of Code\n33 SQLç”Ÿæˆ MIGA: A Unified Multi-task Generation Framework for Conversational Text-to-SQL\nhttps://arxiv.org/pdf/2212.09278.pdf\n34 GPT-4 https://hub.baai.ac.cn/view/24839\n3æœˆ14æ—¥ï¼ŒOpen AIå®˜ç½‘å‘å¸ƒGPT-4ï¼Œæ”¯æŒå›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œæ•ˆæœè¶…è¶ŠChatGPTã€‚\nGPT-4 å®ç°äº†é£è·ƒå¼æå‡ï¼šå¼ºå¤§çš„è¯†å›¾èƒ½åŠ›ï¼›æ–‡å­—è¾“å…¥é™åˆ¶æå‡è‡³ 2.5 ä¸‡å­—ï¼›å›ç­”å‡†ç¡®æ€§æ˜¾è‘—æé«˜ï¼›èƒ½å¤Ÿç”Ÿæˆæ­Œè¯ã€åˆ›æ„æ–‡æœ¬ï¼Œå®ç°é£æ ¼å˜åŒ–ã€‚\nå®˜ç½‘åœ°å€ï¼šhttps://openai.com/product/gpt-4\nè®ºæ–‡ä¸‹è½½ï¼š\nhttps://event-cdn.baai.ac.cn/file/file-browser/BckxAwHQdMdCerFZpXDhhJba6JTWWTZd.pdf\nç›´æ’­åœ°å€ï¼š\nhttps://www.youtube.com/watch?v=outcGtbnMuQ\nè´¡çŒ®è€…ï¼š https://openai.com/contributions/gpt-4\n35 ChatGLMï¼šåƒäº¿åŸºåº§çš„å¯¹è¯æ¨¡å‹å¼€å¯å†…æµ‹ â¸ºå¯¹åº”å•å¡ç‰ˆæœ¬å¼€æº ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº General Language Model (GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 6GB æ˜¾å­˜ï¼‰ã€‚ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ã€‚\nä»£ç é“¾æ¥ï¼šhttps://github.com/THUDM/ChatGLM-6B.git\nå…·ä½“æ¥è¯´ï¼ŒChatGLM-6Bå…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n**å……åˆ†çš„ä¸­è‹±åŒè¯­é¢„è®­ç»ƒï¼š**ChatGLM-6Båœ¨1:1æ¯”ä¾‹çš„ä¸­è‹±è¯­æ–™ä¸Šè®­ç»ƒäº†1Tçš„tokené‡ï¼Œå…¼å…·åŒè¯­èƒ½åŠ›ã€‚ **ä¼˜åŒ–çš„æ¨¡å‹æ¶æ„å’Œå¤§å°ï¼š**å¸å–GLM-130Bè®­ç»ƒç»éªŒï¼Œä¿®æ­£äº†äºŒç»´RoPEä½ç½®ç¼–ç å®ç°ï¼Œä½¿ç”¨ä¼ ç»ŸFFNç»“æ„ã€‚6Bï¼ˆ62äº¿ï¼‰çš„å‚æ•°å¤§å°ï¼Œä¹Ÿä½¿å¾—ç ”ç©¶è€…å’Œä¸ªäººå¼€å‘è€…è‡ªå·±å¾®è°ƒå’Œéƒ¨ç½²ChatGLM-6Bæˆä¸ºå¯èƒ½ã€‚ **è¾ƒä½çš„éƒ¨ç½²é—¨æ§›ï¼š**FP16 åŠç²¾åº¦ä¸‹ï¼ŒChatGLM-6B éœ€è¦è‡³å°‘ 13 GB çš„æ˜¾å­˜è¿›è¡Œæ¨ç†ï¼Œç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œè¿™ä¸€éœ€æ±‚å¯ä»¥è¿›ä¸€æ­¥é™ä½åˆ° 10GBï¼ˆINT8ï¼‰ å’Œ 6GBï¼ˆINT4ï¼‰ï¼Œä½¿å¾— ChatGLM-6B å¯ä»¥éƒ¨ç½²åœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šã€‚ **æ›´é•¿çš„åºåˆ—é•¿åº¦ï¼š**ç›¸æ¯” GLM-10Bï¼ˆåºåˆ—é•¿åº¦1024ï¼‰ï¼ŒChatGLM-6Båºåˆ—é•¿åº¦è¾¾2048ï¼Œæ”¯æŒæ›´é•¿å¯¹è¯å’Œåº”ç”¨ã€‚ **äººç±»æ„å›¾å¯¹é½è®­ç»ƒï¼š**ä½¿ç”¨äº†ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰ã€åé¦ˆè‡ªåŠ©ï¼ˆFeedback Bootstrapï¼‰ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning from Human Feedbackï¼‰ç­‰æ–¹å¼ï¼Œä½¿æ¨¡å‹åˆå…·ç†è§£äººç±»æŒ‡ä»¤æ„å›¾çš„èƒ½åŠ›ã€‚è¾“å‡ºæ ¼å¼ä¸ºmarkdownï¼Œæ–¹ä¾¿å±•ç¤ºã€‚ ä»£ç è°ƒç”¨\nå¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM-6B æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š\n\u003e\u003e\u003e from transformers import AutoTokenizer, AutoModel \u003e\u003e\u003e tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True) \u003e\u003e\u003e model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda() \u003e\u003e\u003e response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[]) \u003e\u003e\u003e print(response) ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚ \u003e\u003e\u003e response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history) \u003e\u003e\u003e print(response) æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•: 1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚ 2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚ 3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚ 4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚ 5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚ 6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚ å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚ 36 å¤šæ¨¡æ€å­¦ä¹  https://zhuanlan.zhihu.com/p/582878508\nå¤šæ¨¡æ€å¯¹è¯ Multimodal Dialogue Response Generation\n10:59\nå›¾åƒï¼‹å°æ¨¡å‹å®ç°æ€ç»´é“¾\nè¿™ä¸€ç¯‡ï¼š Multimodal Chain-of-Thought Reasoning in Language Models å®ƒ å°±æ˜¯è®¾è®¡äº†2ä¸ªç‹¬ç«‹çš„ transformerï¼Œè¯­è¨€çš„å’Œ å›¾åƒçš„ï¼Œç„¶åæŠŠä»–ä»¬åœ¨ decoderçš„è¾“å…¥å±‚åšäº†concatï¼ŒåŸºäºä»»åŠ¡å†åšå¾®è°ƒã€‚ æ²¡å•¥æ–°æ„æ€ï¼Œæ€æƒ³è·Ÿ çŸ¥è¯†åº“é€‚é…å™¨ é‚£ä¸€ç¯‡æ˜¯ä¸€è‡´çš„ã€‚ K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. ï¼ˆå¤šçŸ¥è¯†æºæŒ‚è½½çš„ï¼‰ è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªä¸ºå¤šæ¨¡æ€è®¾è®¡çš„æ¦‚ç‡å»ºæ¨¡æ¡†æ¶ UniDiffuserï¼Œé™¤äº†å•å‘çš„æ–‡ç”Ÿå›¾ï¼Œè¿˜èƒ½å®ç°å›¾ç”Ÿæ–‡ã€å›¾æ–‡è”åˆç”Ÿæˆã€æ— æ¡ä»¶å›¾æ–‡ç”Ÿæˆã€å›¾æ–‡æ”¹å†™ç­‰å¤šç§åŠŸèƒ½ã€‚\næ®æ‚‰ GPT-4 å°†äºæœ¬å‘¨å‘å¸ƒï¼Œå¤šæ¨¡æ€å°†æˆä¸ºå…¶ä¸€å¤§äº®ç‚¹ã€‚å½“å‰çš„å¤§è¯­è¨€æ¨¡å‹æ­£åœ¨æˆä¸ºç†è§£å„ç§æ¨¡æ€çš„é€šç”¨æ¥å£ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒæ¨¡æ€ä¿¡æ¯æ¥ç»™å‡ºå›å¤æ–‡æœ¬ï¼Œä½†å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ä¹Ÿä»…ä»…å±€é™äºæ–‡æœ¬ã€‚å¦ä¸€æ–¹é¢ï¼Œå½“å‰çš„æ‰©æ•£æ¨¡å‹ DALLãƒ»E 2ã€Imagenã€Stable Diffusion ç­‰åœ¨è§†è§‰åˆ›ä½œä¸Šæ€èµ·ä¸€åœºé©å‘½ï¼Œä½†è¿™äº›æ¨¡å‹ä»…ä»…æ”¯æŒæ–‡åˆ°å›¾çš„å•ä¸€è·¨æ¨¡æ€åŠŸèƒ½ï¼Œç¦»é€šç”¨å¼ç”Ÿæˆæ¨¡å‹è¿˜æœ‰ä¸€å®šè·ç¦»ã€‚è€Œå¤šæ¨¡æ€å¤§æ¨¡å‹å°†èƒ½å¤Ÿæ‰“é€šå„ç§æ¨¡æ€èƒ½åŠ›ï¼Œå®ç°ä»»æ„æ¨¡æ€ä¹‹é—´è½¬åŒ–ï¼Œè¢«è®¤ä¸ºæ˜¯é€šç”¨å¼ç”Ÿæˆæ¨¡å‹çš„æœªæ¥å‘å±•æ–¹å‘ã€‚\næ¸…åå¤§å­¦è®¡ç®—æœºç³»æœ±å†›æ•™æˆå¸¦é¢†çš„ TSAIL å›¢é˜Ÿè¿‘æœŸå…¬å¼€çš„ä¸€ç¯‡è®ºæ–‡ã€ŠOne Transformer Fits All Distributions in Multi-Modal Diffusion at Scaleã€‹ï¼Œç‡å…ˆå‘å¸ƒäº†å¯¹å¤šæ¨¡æ€ç”Ÿæˆå¼æ¨¡å‹çš„ä¸€äº›æ¢ç´¢å·¥ä½œï¼Œå®ç°äº†ä»»æ„æ¨¡æ€ä¹‹é—´çš„ç›¸äº’è½¬åŒ–ã€‚\nè®ºæ–‡é“¾æ¥ï¼š https://ml.cs.tsinghua.edu.cn/diffusion/unidiffuser.pdf\nå¼€æºä»£ç ï¼š https://github.com/thu-ml/unidiffuser\n",
  "wordCount" : "2200",
  "inLanguage": "en",
  "datePublished": "2023-07-07T11:49:51+08:00",
  "dateModified": "2023-07-07T11:49:51+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://example.org/posts/my-first-post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My New Hugo Site",
    "logo": {
      "@type": "ImageObject",
      "url": "http://example.org/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://example.org/" accesskey="h" title="My New Hugo Site (Alt + H)">My New Hugo Site</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™
    </h1>
    <div class="post-meta"><span title='2023-07-07 11:49:51 +0800 CST'>July 7, 2023</span>

</div>
  </header> 
  <div class="post-content"><h1 id="åŸºäºdlçš„nlpæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™v10222">åŸºäºDLçš„NLPæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™v1.0222<a hidden class="anchor" aria-hidden="true" href="#åŸºäºdlçš„nlpæŠ€æœ¯è·¯çº¿åˆ†æåŠç›¸å…³ææ–™v10222">#</a></h1>
<p><strong>å£°æ˜:æœ¬æ–‡æ¡£å†…å®¹ä»…æ˜¯ä¸ªäººç†è§£ï¼Œä¸å¯¹æœ¯è¯­çš„ä¸“ä¸šæ€§è´Ÿè´£ï¼Œä»…ä¾›å‚è€ƒã€‚</strong></p>
<h2 id="1-dl-nlpæŠ€æœ¯èŒƒå¼ç®€å²">1 DL NLPæŠ€æœ¯èŒƒå¼ç®€å²<a hidden class="anchor" aria-hidden="true" href="#1-dl-nlpæŠ€æœ¯èŒƒå¼ç®€å²">#</a></h2>
<p>æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŠ€æœ¯èŒƒå¼å¯åˆ†ä¸ºä»¥ä¸‹å››ä¸ªé˜¶æ®µï¼š</p>
<p><strong>ç¬¬ä¸€é˜¶æ®µï¼Œè¯/å­—å‘é‡+ç‰¹å®šä»»åŠ¡ç½‘ç»œç»“æ„è®¾è®¡</strong></p>
<p>ä¾§é‡ç‚¹ï¼šç‰¹å®šä»»åŠ¡çš„ç½‘ç»œè®¾è®¡+æ•°æ®å·¥ç¨‹</p>
<p>ä»£è¡¨æ¨¡å‹: é˜…è¯»ç†è§£ä»»åŠ¡çš„<code>BiDAF</code>ã€<code>QANET</code>;æ–‡æœ¬åŒ¹é…çš„<code>DSSM</code>ã€<code>ARC-I</code>;æœºå™¨ç¿»è¯‘çš„<code>nmt</code>ç­‰</p>
<p><strong>ç¬¬äºŒé˜¶æ®µï¼ŒåŸºäºBERT+ç‰¹å®šä»»åŠ¡è¾“å‡ºå±‚ç½‘ç»œç»“æ„è®¾è®¡</strong></p>
<p>ä¾§é‡ç‚¹ï¼šç›®æ ‡å‡½æ•°çš„è®¾è®¡+æ•°æ®å·¥ç¨‹</p>
<p>ä»£è¡¨æ¨¡å‹ï¼š<a href="https://arxiv.org/pdf/1810.04805.pdf"><code>BERT</code></a>ã€<a href="https://arxiv.org/pdf/2004.13922.pdf"><code>MacBert</code></a>ã€<a href="https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf"><code>XLNET</code></a>ç­‰</p>
<p>å‡ºå‘ç‚¹ï¼š</p>
<ul>
<li>è¯­ä¹‰ä»»åŠ¡çš„è§£å†³ï¼Œéœ€è¦å¤æ‚çš„ç½‘ç»œï¼›</li>
<li>å¤æ‚çš„ç½‘ç»œï¼Œéœ€è¦æ›´é«˜çš„æ ‡æ³¨æ•°æ®æˆæœ¬ï¼›</li>
<li>é‡‡ç”¨æœ‰ç›‘ç£çš„è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥é™ä½æ•°æ®æˆæœ¬å¹¶æå‡æ€§èƒ½ã€‚</li>
</ul>
<p><strong>ç¬¬ä¸‰é˜¶æ®µï¼ŒåŸºäº<code>LLM</code>(å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹)çš„é¢å‘ç›®æ ‡ä»»åŠ¡å½¢å¼promptå·¥ç¨‹</strong></p>
<p>ä¾§é‡ç‚¹:promptå·¥ç¨‹+å¤šä»»åŠ¡å­¦ä¹ +æ•°æ®å·¥ç¨‹</p>
<p>ä»£è¡¨æ¨¡å‹:<a href=""><code>GODEL</code></a>ã€<a href="https://universal-ie.github.io/"><code>UIE</code></a>ç­‰</p>
<p>å‡ºå‘ç‚¹ï¼š</p>
<ul>
<li>ç›®æ ‡ä»»åŠ¡æ²¡æœ‰å¾ˆå¥½åœ°åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„æ€§èƒ½</li>
<li><code>LLM</code>æ¨¡å‹å¾®è°ƒæˆæœ¬é«˜</li>
<li>ä»é€‚åº”ä¸‹æ¸¸ä»»åŠ¡è½¬å˜ä¸ºé€‚åº”è¯­è¨€æ¨¡å‹ï¼Œä»¥å®ç°å¤šä»»åŠ¡å­¦ä¹ çš„ä¸€èˆ¬åŒ–ï¼ˆåˆå:ç»Ÿä¸€å»ºæ¨¡ï¼‰</li>
</ul>
<p><strong>ç¬¬å››é˜¶æ®µï¼Œé¢å‘<code>instruct</code>çš„<code>LLM</code>çš„ä»»åŠ¡å­¦ä¹ èƒ½åŠ›</strong></p>
<p>ä¾§é‡ç‚¹:ä»»åŠ¡è¿ç§»+å®‰å…¨æ€§+æ•°æ®å·¥ç¨‹</p>
<p>ä»£è¡¨æ¨¡å‹ï¼š<a href="https://arxiv.org/abs/2110.08207"><code>T0</code></a>ã€<a href="https://arxiv.org/abs/2109.01652"><code>FLAN</code></a>ã€<a href="https://arxiv.org/abs/2203.02155"><code>instructGPT</code></a>ã€<a href="https://export.arxiv.org/pdf/2204.07705v3.pdf">SUP-NATINST</a>ç­‰</p>
<p>å‡ºå‘ç‚¹ï¼š</p>
<ul>
<li>è¯­è¨€æ¨¡å‹å…·å¤‡è·¨ä»»åŠ¡èƒ½åŠ›è¿ç§»çš„èƒ½åŠ›</li>
<li>è·¨ä»»åŠ¡çš„èƒ½åŠ›ä¹ å¾—éœ€è¦æ›´å¤šçš„æ•°æ®ï¼ˆç›¸æ¯”äºé¢„è®­ç»ƒæ•°æ®è§„æ¨¡å¤ªå°äº†ï¼‰</li>
<li>æ•°æ®å¢å¼ºçš„ä¸¤ä¸ªæ€è·¯ï¼š
<ul>
<li>åŸºäºinstructå­¦ä¹ çš„æ–¹å¼é‡æ„æ›´å¤šçš„ä»»åŠ¡ï¼Œå¦‚<code>SUP-NATINST</code></li>
<li>ç›´æ¥åŸºäºinstructè¿›è¡Œä»¿äººç±»å›å¤ï¼Œå¦‚<code>instructGPT</code></li>
</ul>
</li>
<li>å¯¹AIç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§æå‡ºè¦æ±‚</li>
</ul>
<h2 id="2-è¯­è¨€æ¨¡å‹">2 è¯­è¨€æ¨¡å‹<a hidden class="anchor" aria-hidden="true" href="#2-è¯­è¨€æ¨¡å‹">#</a></h2>
<ul>
<li>
<p><strong>æ¦‚è¿°è®ºæ–‡</strong></p>
</li>
<li>
<p>A Survey of Transformers [2021]</p>
<p><a href="https://export.arxiv.org/pdf/2106.04554.pdf">https://export.arxiv.org/pdf/2106.04554.pdf</a></p>
</li>
<li>
<p>Efficient Transformers: A Survey [2020]</p>
<p><a href="http://export.arxiv.org/pdf/2009.06732v3.pdf">http://export.arxiv.org/pdf/2009.06732v3.pdf</a></p>
</li>
<li>
<p>Pre-Trained Models: Past, Present and Future [2021]</p>
<p><a href="https://export.arxiv.org/pdf/2106.07139.pdf">https://export.arxiv.org/pdf/2106.07139.pdf</a></p>
</li>
<li>
<p>A Survey on Knowledge-Enhanced Pre-trained Language Models [2022]</p>
<p><a href="https://export.arxiv.org/pdf/2212.13428v1.pdf">https://export.arxiv.org/pdf/2212.13428v1.pdf</a></p>
</li>
<li>
<p>Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey [2021]</p>
<p><a href="https://export.arxiv.org/pdf/2111.01243.pdf">https://export.arxiv.org/pdf/2111.01243.pdf</a></p>
</li>
<li>
<p>LARGE LANGUAGE MODELS ARE HUMAN-LEVELPROMPT ENGINEERS [2022]</p>
<p><a href="https://export.arxiv.org/pdf/2211.01910v1.pdf">https://export.arxiv.org/pdf/2211.01910v1.pdf</a></p>
</li>
<li>
<p>A Survey on Transformers in Reinforcement Learning [2023]</p>
<p><a href="https://export.arxiv.org/pdf/2301.03044v1.pdf">https://export.arxiv.org/pdf/2301.03044v1.pdf</a></p>
</li>
<li>
<p>Transformers in Vision: A Survey [2022]</p>
<p><a href="https://arxiv.org/abs/2101.01169v5">https://arxiv.org/abs/2101.01169v5</a></p>
</li>
<li>
<p>Transformeræ¨¡å‹å¤§ç›˜ç‚¹2023ç‰ˆï½œå‰Quoraå·¥ç¨‹å‰¯æ€»è£ï¼ˆè¶…é•¿ï¼‰</p>
<p><a href="https://hub.baai.ac.cn/view/24175">https://hub.baai.ac.cn/view/24175</a></p>
<p><a href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/">https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/</a></p>
</li>
<li>
<p><strong>ä»£è¡¨è®ºæ–‡</strong></p>
</li>
<li>
<p>Language Models are Few-Shot Learners</p>
<p><a href="https://arxiv.org/pdf/2005.14165.pdf">https://arxiv.org/pdf/2005.14165.pdf</a></p>
</li>
<li>
<p>**(å¾ˆé‡è¦)**T5:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p>
</li>
</ul>
<p>â€‹	https://arxiv.org/abs/1910.10683</p>
<p>â€‹	https://github.com/google-research/text-to-text-transfer-transformer</p>
<blockquote>
<p>The <a href="https://www.tensorflow.org/datasets/catalog/c4">C4</a> dataset we created for unsupervised pre-training is available in TensorFlow Datasets, but it 	requires a significant amount of bandwidth for downloading the raw <a href="https://commoncrawl.org/">Common Crawl</a> scrapes (~7 TB) and compute for its preparation (~335 CPU-days)</p>
</blockquote>
<ul>
<li>
<p>ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning</p>
<p><a href="https://arxiv.org/abs/2111.10952">https://arxiv.org/abs/2111.10952</a></p>
</li>
<li>
<p><strong>å¼€æºæ¨¡å‹æˆ–ä»£ç </strong></p>
</li>
<li>
<p>ChatYuan: å…ƒè¯­åŠŸèƒ½å‹å¯¹è¯å¤§æ¨¡å‹</p>
<p><a href="https://github.com/clue-ai/ChatYuan">https://github.com/clue-ai/ChatYuan</a></p>
<p><a href="https://mp.weixin.qq.com/s/-axa6XcjGl_Koeq_OrDq8w">https://mp.weixin.qq.com/s/-axa6XcjGl_Koeq_OrDq8w</a></p>
<p><a href="https://github.com/clue-ai/PromptCLUE">https://github.com/clue-ai/PromptCLUE</a></p>
<p><a href="https://github.com/clue-ai/clueai-python">https://github.com/clue-ai/clueai-python</a></p>
</li>
<li>
<p>ChatYuan-v2</p>
<pre tabindex="0"><code>20230324æ›´æ–°ï¼š
ChatYuanå¼€æºæ¨¡å‹å¤§å‡çº§ï¼Œæ•ˆæœæ˜æ˜¾æå‡
ChatYuan-large-v2æ˜¯ChatYuanç³»åˆ—ä¸­ä»¥è½»é‡åŒ–å®ç°é«˜è´¨é‡æ•ˆæœçš„æ¨¡å‹ä¹‹ä¸€ï¼Œæ”¯æŒè¾“å…¥è¾“å‡ºæ€»é•¿åº¦æœ€é•¿4kï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§æ˜¾å¡ã€ PCç”šè‡³æ‰‹æœºä¸Šè¿›è¡Œæ¨ç†ï¼ˆINT4 æœ€ä½åªéœ€ 400M ï¼‰ã€‚
æ¬¢è¿ä½“éªŒä½¿ç”¨
1. å¼€æºé¡¹ç›®ï¼šhttps://github.com/clue-ai/ChatYuan
2. å¼€æºæ¨¡å‹hfåœ°å€ï¼šhttps://huggingface.co/ClueAI/ChatYuan-large-v2
3. å¼€æºæ¨¡å‹msåœ°å€ï¼šhttps://modelscope.cn/models/ClueAI/ChatYuan-large-v2/summary
4. æ¨¡å‹hfä½“éªŒåœ°å€ï¼ˆæ¨èï¼‰ï¼šhttps://huggingface.co/spaces/ClueAI/ChatYuan-large-v2
5. æ¨¡å‹msä½“éªŒåœ°å€ï¼ˆæš‚æ—¶æœ‰é—®é¢˜ï¼‰ï¼šhttps://modelscope.cn/studios/ClueAI/ChatYuan-large-v2/summary
</code></pre></li>
<li>
<p>GPTç›¸å…³
<a href="https://github.com/Morizeyao/GPT2-Chinese">https://github.com/Morizeyao/GPT2-Chinese</a></p>
<p><a href="https://github.com/Guhaifudeng/gpt-2">https://github.com/Guhaifudeng/gpt-2</a></p>
</li>
<li>
<p>T5è‹±æ–‡ç‰ˆ
<code>T5</code>ç¬¬ä¸€ç‰ˆæœ¬ä»…æ”¯æŒè‹±æ–‡ï¼Œåé€€å‡ºå¤šè¯­è¨€ç‰ˆæœ¬<code>mT5</code>&ndash;è§‚å¯Ÿè¯è¡¨</p>
<p><a href="https://huggingface.co/t5-large">https://huggingface.co/t5-large</a></p>
<p><a href="https://huggingface.co/google/mt5-large">https://huggingface.co/google/mt5-large</a></p>
<p><a href="https://huggingface.co/google/flan-t5-large">https://huggingface.co/google/flan-t5-large</a></p>
<p><a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md">https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md</a></p>
</li>
<li>
<p>T5ä¸­æ–‡ç‰ˆæœ¬
T5ä¸­æ–‡æ›¿ä»£å“:</p>
<p><a href="http://jmlr.org/papers/v21/20-074.html">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></p>
<p><a href="https://huggingface.co/IDEA-CCNL/Randeng-T5-784M">https://huggingface.co/IDEA-CCNL/Randeng-T5-784M</a>ã€€<a href="https://github.com/IDEA-CCNL/Fengshenbang-LM">é¡¹ç›®åœ°å€</a></p>
<p><a href="https://huggingface.co/TsinghuaAI/CPM-Generate">https://huggingface.co/TsinghuaAI/CPM-Generate</a></p>
<p><a href="https://huggingface.co/IDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese">https://huggingface.co/IDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese</a></p>
<p><a href="https://huggingface.co/IDEA-CCNL/Randeng-T5-784M-QA-Chinese">https://huggingface.co/IDEA-CCNL/Randeng-T5-784M-QA-Chinese</a></p>
<p><a href="https://github.com/IDEA-CCNL/Fengshenbang-LM/tree/main/fengshen/examples/qa_t5">https://github.com/IDEA-CCNL/Fengshenbang-LM/tree/main/fengshen/examples/qa_t5</a></p>
<p><a href="https://github.com/bojone/t5_in_bert4keras">https://github.com/bojone/t5_in_bert4keras</a></p>
<p><a href="https://arxiv.org/abs/1912.08777">https://arxiv.org/abs/1912.08777</a>
<a href="https://github.com/shuxinyin/T5-NLP">https://github.com/shuxinyin/T5-NLP</a></p>
</li>
<li>
<p><em>BART</em>ç›¸å…³</p>
<p>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</p>
<p><a href="https://huggingface.co/fnlp/bart-large-chinese">https://huggingface.co/fnlp/bart-large-chinese</a></p>
<p><a href="https://github.com/fastnlp/CPT">https://github.com/fastnlp/CPT</a></p>
<p><a href="https://github.com/facebookresearch/fairseq/tree/main/examples/bart">https://github.com/facebookresearch/fairseq/tree/main/examples/bart</a></p>
</li>
<li>
<p><strong>ï¼ˆé‡è¦ï¼‰çº¿æ€§è¯­è¨€æ¨¡å‹RWKV</strong>
<strong>é‡‡ç”¨RNNå®ç°ï¼Œæ•ˆç‡å’Œæ€§èƒ½æ¥è¿‘äºçº¿æ€§Transformer</strong>
<a href="https://github.com/BlinkDL/RWKV-LM">https://github.com/BlinkDL/RWKV-LM</a> (explanation, fine-tuning, training, etc.)
RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling</p>
<p><a href="https://github.com/dreasysnail/RetGen">https://github.com/dreasysnail/RetGen</a></p>
<p><a href="https://pypi.org/project/rwkvstic/">https://pypi.org/project/rwkvstic/</a> Easy pip package (with 8bit &amp; offload for low VRAM GPUs)</p>
<p><a href="https://github.com/harrisonvanderbyl/rwkv_chatbot">https://github.com/harrisonvanderbyl/rwkv_chatbot</a> Chatbot using rwkvstic</p>
<p><a href="https://github.com/gururise/rwkv_gradio">https://github.com/gururise/rwkv_gradio</a> RWKV Gradio</p>
<p><a href="https://github.com/mrsteyk/RWKV-LM-deepspeed">https://github.com/mrsteyk/RWKV-LM-deepspeed</a> Another training fork</p>
<p><a href="https://github.com/Blealtan/RWKV-LM-LoRA">https://github.com/Blealtan/RWKV-LM-LoRA</a> LoRA fine-tuning</p>
<p><a href="https://github.com/wozeparrot/tinyrwkv">https://github.com/wozeparrot/tinyrwkv</a> RWKV in tinygrad (nice simple DL framework)</p>
<p>huggingface/transformers#17230 RWKV HF package (WIP)</p>
<p><a href="https://github.com/ArEnSc/Production-RWKV">https://github.com/ArEnSc/Production-RWKV</a> RWKV HF package source</p>
<p><a href="https://github.com/nlpodyssey/verbaflow">https://github.com/nlpodyssey/verbaflow</a> RWKV in Go</p>
<p><a href="https://github.com/nlpodyssey/rwkv">https://github.com/nlpodyssey/rwkv</a> RWKV in Go</p>
<p><a href="https://github.com/mrsteyk/rwkvk-rs">https://github.com/mrsteyk/rwkvk-rs</a> RWKV in Rust</p>
<p><a href="https://github.com/imxcstar/CSharp-RWKV-V4">https://github.com/imxcstar/CSharp-RWKV-V4</a> RWKV in<code> C#</code></p>
<p><a href="https://github.com/resloved/RWKV-notebooks">https://github.com/resloved/RWKV-notebooks</a> RWKV colab notebooks</p>
<p><a href="https://colab.research.google.com/github/harrisonvanderbyl/rwkvstic/blob/master/notebooks/chatbot.ipynb">https://colab.research.google.com/github/harrisonvanderbyl/rwkvstic/blob/master/notebooks/chatbot.ipynb</a> RWKV chatbot colab notebook</p>
<p><a href="https://github.com/Pathos14489/RWKVDistributedInference">https://github.com/Pathos14489/RWKVDistributedInference</a> RWKV Distributed Inference</p>
<p><a href="https://github.com/AXKuhta/rwkv-onnx-dml">https://github.com/AXKuhta/rwkv-onnx-dml</a> RWKV ONNX</p>
<p><a href="https://github.com/josephrocca/rwkv-v4-web">https://github.com/josephrocca/rwkv-v4-web</a> RWKV-v4 running in the browser (simple demo. greedy decode)</p>
</li>
</ul>
<h2 id="3-è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§">3 è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§<a hidden class="anchor" aria-hidden="true" href="#3-è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§">#</a></h2>
<ul>
<li>
<p>Language Models as Knowledge Bases?</p>
<p><a href="https://github.com/facebookresearch/LAMA">https://github.com/facebookresearch/LAMA</a></p>
</li>
<li>
<p><strong>How Can We Know What Language Models Know?</strong></p>
<p><a href="https://github.com/jzbjyb/LPAQA">https://github.com/jzbjyb/LPAQA</a></p>
</li>
<li>
<p><strong>Making Pre-trained Language Models Better Few-shot Learners</strong></p>
</li>
<li>
<p><strong>What Makes Good In-Context Examples for GPT-$3$?</strong></p>
</li>
<li>
<p><strong>çŸ¥æ™“T5-Largeä¸èƒ½åšä»€ä¹ˆ</strong></p>
</li>
<li>
<p><strong>(é‡è¦-ç”Ÿæˆæç¤º) LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯äººç±»çº§åˆ«çš„æç¤ºå·¥ç¨‹å¸ˆ:è€ƒè™‘é‡‡ç”¨chatGPTæ‰©å±•æç¤º</p>
</li>
<li>
<p>TextBox: A Unified, Modularized, and Extensible Framework for Text Generation
<a href="https://github.com/RUCAIBox/TextBox">https://github.com/RUCAIBox/TextBox</a></p>
</li>
<li>
<p>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</p>
<p><a href="https://export.arxiv.org/pdf/2202.12837.pdf">https://export.arxiv.org/pdf/2202.12837.pdf</a></p>
</li>
</ul>
<h2 id="4-promptå­¦ä¹ èŒƒå¼">4 promptå­¦ä¹ èŒƒå¼<a hidden class="anchor" aria-hidden="true" href="#4-promptå­¦ä¹ èŒƒå¼">#</a></h2>
<ul>
<li>
<p><strong>æ¦‚è¿°è®ºæ–‡</strong></p>
</li>
<li>
<p><strong>(é‡è¦)Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</strong>
<a href="http://pretrain.nlpedia.ai/">http://pretrain.nlpedia.ai/</a></p>
</li>
<li>
<p><strong>ä»£è¡¨è®ºæ–‡</strong></p>
</li>
<li>
<p><strong>(é‡è¦)The Power of Scale for Parameter-Efficient Prompt Tuning</strong></p>
<p><a href="https://arxiv.org/abs/2104.08691">https://arxiv.org/abs/2104.08691</a>
<a href="https://github.com/kipgparker/soft-prompt-tuning">https://github.com/kipgparker/soft-prompt-tuning</a></p>
<p>è§£è¯»:https://code84.com/745392.html</p>
<p>åœ¨å…¨é‡æ•°æ®æƒ…å†µä¸‹ï¼Œä»…å¾®è°ƒ prompt ç›¸å…³çš„å‚æ•°ï¼Œèƒ½å¦åª²ç¾ç”šè‡³è¶…è¿‡ fine-tuning çš„è¡¨ç°ï¼Ÿ</p>
<p>åœ¨å°‘é‡æ•°æ®æƒ…å†µä¸‹ï¼Œä»…å¾®è°ƒ prompt ç›¸å…³çš„å‚æ•°ï¼Œèƒ½å¦åª²ç¾ç”šè‡³è¶…è¿‡ fine-tuning çš„è¡¨ç°ï¼Ÿ</p>
<p>å¦‚æœèƒ½åšåˆ°ä¸Šè¿°è¡¨ç°ï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„å°ºå¯¸æ˜¯å¦æœ‰å½±å“ï¼Ÿæ˜¯å¦ä¸€å®šéœ€è¦è¶…å¤§é¢„è®­ç»ƒæ¨¡å‹ï¼Ÿ</p>
<p>ç»“è®ºï¼š</p>
<ul>
<li>prompt tokensé€‰æ‹©20è¯å·¦å³</li>
<li>æ„å»ºå¥½promtæŒ‰ç…§è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡è®­ç»ƒ50Kï¼Œç”šè‡³100Kæ­¥</li>
<li>prompt tokençš„åˆå§‹åŒ–å¯¹ç»“æœå½±å“å¾ˆå¤§</li>
<li>ä»¥ä¸Šç­–ç•¥åœ¨1Bçº§åˆ«åŠä»¥ä¸‹æœ‰æ•ˆï¼Œè¶…è¿‡10Bï¼Œå½±å“å˜å°</li>
</ul>
</li>
<li>
<p>PPT: Pre-trained Prompt Tuning for Few-shot Learning</p>
<p><a href="https://arxiv.org/abs/2109.04332">https://arxiv.org/abs/2109.04332</a></p>
</li>
<li>
<p>SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer</p>
<p><a href="https://arxiv.org/abs/2110.07904">https://arxiv.org/abs/2110.07904</a></p>
</li>
<li>
<p>ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization]</p>
<p><a href="https://arxiv.org/abs/2201.06910">https://arxiv.org/abs/2201.06910</a></p>
</li>
<li>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p>
<p><a href="https://arxiv.org/abs/1910.10683">https://arxiv.org/abs/1910.10683</a></p>
</li>
<li>
<p>Unified Structure Generation for Universal Information Extraction</p>
<p><a href="https://arxiv.org/abs/2203.12277">https://arxiv.org/abs/2203.12277</a></p>
</li>
<li>
<p>Multitask Prompted Training Enables Zero-Shot Task Generalization</p>
<p><a href="https://arxiv.org/abs/2110.08207">https://arxiv.org/abs/2110.08207</a></p>
</li>
<li>
<p>Learning To Retrieve Prompts for In-Context Learning</p>
</li>
<li>
<p><strong>å¼€æºæ¨¡å‹åŠä»£ç </strong></p>
</li>
<li>
<p><strong>OpenPrompt: An Open-source Framework for Prompt-learning</strong></p>
</li>
<li>
<p><strong>(é‡è¦)Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation</strong></p>
</li>
<li>
<p>è‡ªåŠ¨ç”ŸæˆPrompt
<a href="https://github.com/princeton-nlp/LM-BFF">https://github.com/princeton-nlp/LM-BFF</a></p>
</li>
<li>
<p>ç”Ÿæˆå¼ä»»åŠ¡å°è¯•</p>
<p><a href="https://github.com/clue-ai/PromptCLUE">https://github.com/clue-ai/PromptCLUE</a></p>
<ol>
<li>
<p>ä¸‰å¤§ç»Ÿä¸€ï¼šç»Ÿä¸€æ¨¡å‹æ¡†æ¶(text-to-text)ï¼Œç»Ÿä¸€ä»»åŠ¡å½¢å¼(prompt)ï¼Œç»Ÿä¸€åº”ç”¨æ–¹å¼(zero-shot/few-shot)ã€‚ (<a href="https://arxiv.org/abs/2110.08207">T0</a>ï¼‰</p>
</li>
<li>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒï¼šåœ¨t5-largeç‰ˆåŸºç¡€ä¸Šï¼Œä½¿ç”¨æ•°ç™¾Gä¸­æ–‡è¯­æ–™ï¼Œè®­ç»ƒäº†100ä¸‡æ­¥ï¼Œç´¯ç§¯è®­ç»ƒäº†1.5ä¸‡äº¿ä¸ªä¸­æ–‡å­—è¯çº§åˆ«token</p>
</li>
<li>
<p>å¤§è§„æ¨¡ä»»åŠ¡æ•°æ®ï¼šä½¿ç”¨äº†16ç§ä»»åŠ¡ç±»å‹ï¼Œæ•°ç™¾ç§ä»»åŠ¡ï¼Œç´¯ç§¯äº¿çº§åˆ«ä»»åŠ¡æ•°æ®ã€‚</p>
</li>
<li>
<p>æ··åˆé¢„è®­ç»ƒï¼šä¸€æ–¹é¢å°†ä¸‹æ¸¸ä»»åŠ¡ä½œä¸ºé¢„è®­ç»ƒè¯­æ–™ï¼Œå¦ä¸€æ–¹é¢å°†ä¸‹æ¸¸ä»»åŠ¡å’Œé¢„è®­ç»ƒè¯­æ–™ä¸€èµ·è®­ç»ƒï¼Œå‡å°‘ä»»åŠ¡ç¾éš¾é—å¿˜ä»¥åŠç¼©çŸ­é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡çš„è·ç¦»ï¼Œæ›´å¥½çš„é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼ˆ<a href="https://arxiv.org/abs/2111.10952">ExT5</a>ï¼‰</p>
</li>
<li>
<p>æ··åˆé‡‡æ ·ï¼šé’ˆå¯¹ä¼—å¤šæ•°æ®é‡å·®å¼‚æå¤§çš„ä»»åŠ¡ï¼Œé‡‡ç”¨åœ¨æ¯ä¸ªè®­ç»ƒbatchå†…å¯¹æ‰€æœ‰çš„ä»»åŠ¡è¿›è¡ŒæŒ‰ç…§æ¯”ä¾‹é‡‡æ ·ï¼Œæ ¹æ®ä»»åŠ¡çš„æ•°æ®é‡è¿›è¡Œå¹³æ»‘é‡‡æ ·ï¼Œå¹¶ä¸”åŒæ—¶é™åˆ¶ä»»åŠ¡æ•°æ®é‡é‡‡æ ·æ± çš„ä¸Šé™ã€‚ å¹³æ»‘é‡‡æ ·å¯ä»¥å‡å°‘ä»»åŠ¡è®­ç»ƒæœ‰åå±å®³ï¼Œåœ¨æ¯ä¸€batchå†…è®­ç»ƒå¯ä»¥å‡å°‘å¼‚è´¨ä»»åŠ¡ä¹‹é—´è®­ç»ƒè´Ÿè¿ç§»çš„æƒ…å†µ(T5)</p>
</li>
<li>
<p>åˆ†é˜¶æ®µè®­ç»ƒï¼šä¸€æ–¹é¢æŒ‡åœ¨é¢„è®­ç»ƒåˆ†é˜¶æ®µï¼Œæ¶‰åŠè®­ç»ƒåºåˆ—é•¿åº¦çš„åˆ†é˜¶æ®µï¼ˆ128å’Œ512ï¼‰ï¼ŒåŠ å¿«é¢„è®­ç»ƒé€Ÿåº¦(Bert)ï¼›å¦ä¸€æ–¹é¢ï¼Œåœ¨ä¸‹æ¸¸è®­ç»ƒåˆ†é˜¶æ®µï¼Œ æ¶‰åŠå­¦ä¹ ç‡å’Œåºåˆ—é•¿åº¦çš„å˜åŒ–ä»¥åŠé€’å‡å¼å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®é‡é™åˆ¶ï¼Œæ›´å¥½çš„é€‚åº”ä¸‹æ¸¸çš„ä¸åŒä»»åŠ¡</p>
</li>
<li>
<p>å¢åŠ è¯­è¨€æ¨¡å‹çš„è®­ç»ƒï¼šå‚è€ƒt5.1.1, é™¤äº†ä½¿ç”¨Span Corrputionæ„å»ºçš„æ–¹å¼è¿›è¡Œæ— ç›‘ç£è®­ç»ƒï¼ŒåŒæ—¶åœ¨ä½¿ç”¨prefix LMçš„æ–¹å¼è®­ç»ƒï¼Œå¢å¼ºç”Ÿæˆä»»åŠ¡çš„èƒ½åŠ›(<a href="https://arxiv.org/abs/1910.10683">LM adapted</a>)</p>
</li>
<li>
<p>å¢åŠ å¯¹æ¨¡å‹çš„encoderä»¥åŠdecoderçš„è®­ç»ƒï¼šæ ¹æ®ä¸‹æ¸¸ä»»åŠ¡æ•°æ®åˆ†åˆ«æ„å»ºData_text,Data_targeté¢„è®­ç»ƒæ•°æ®è¯­æ–™ï¼Œæ˜¯åŠ å…¥åˆ°é¢„è®­ç»ƒä¸­ï¼Œåˆ†åˆ«å¢å¼ºæ¨¡å‹çš„encoderç†è§£èƒ½åŠ›å’Œ decoderçš„ç”Ÿæˆèƒ½åŠ›ï¼ˆè§<a href="https://arxiv.org/abs/2203.12277">UIE</a>ï¼‰</p>
</li>
<li>
<p>é‡æ–°æ„å»ºæ¨¡å‹ä¸­æ–‡å­—å…¸ï¼šä½¿ç”¨sentencepieceä¸Šåœ¨åƒäº¿tokenä¸Šå­¦ä¹ å¹¶æ„å»ºæ¨¡å‹å­—å…¸ï¼Œæ›´åŠ ç¬¦åˆä¸­æ–‡è¯­è¨€ä¹ æƒ¯</p>
</li>
</ol>
</li>
<li>
<p><strong>å…¶ä»–</strong></p>
</li>
<li>
<p><strong>æ‰©å±•è®ºæ–‡æ±‡æ€»</strong></p>
<ul>
<li>åœ°å€:github.com/thunlp/PromptPapers</li>
</ul>
</li>
<li>
<p>NLPçš„â€œç¬¬å››èŒƒå¼â€ä¹‹Prompt Learningæ€»ç»“ï¼š44ç¯‡è®ºæ–‡é€ä¸€æ¢³ç†</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247543504&amp;idx=2&amp;sn=ed724e69ddffe060171816156921b0eb&amp;chksm=96eaf550a19d7c46d1b899f6bc038ec463517d758a6530531ffa0a53c398ce9d038d248f9269&amp;scene=21#wechat_redirect">åœ°å€</a></p>
<p>åŸºäºpromptçš„æ–°è®­ç»ƒèŒƒå¼</p>
<ol>
<li>
<p>ç›¸æ¯”ä¹‹å‰æ¯ä¸ªä»»åŠ¡å®šä¹‰ä¸€å¥—å‚æ•°ï¼Œåœ¨è¾“å…¥åŠ ä¸Šç‰¹å®šçš„ä¿¡æ¯ï¼Œä¸éœ€è¦æ”¹å˜æ•´ä¸ªæ¨¡å‹çš„å‚æ•°ï¼Œä»è€Œæå‡æ•ˆç‡å’Œå­˜å‚¨ç©ºé—´ã€‚</p>
</li>
<li>
<p>ä¼ ç»Ÿ pretrain+fintune çš„è®­ç»ƒæ–¹å¼æ˜¯æœ‰ gap çš„ï¼Œéœ€è¦ä»å¤§è§„æ¨¡æ— ç›‘ç£æ•°æ®è®­ç»ƒè¿ç§»åˆ°ä¸‹æ¸¸ finetune çš„ä»»åŠ¡ï¼Œprompt-based çš„æ–¹å¼æ‰“ç ´äº†è¿™ä¸ªæ–¹å¼ã€‚</p>
</li>
</ol>
</li>
<li>
<p>Promptæ–¹æ³•ç»¼è¿°</p>
<p><a href="https://zhuanlan.zhihu.com/p/431788068">https://zhuanlan.zhihu.com/p/431788068</a></p>
</li>
<li>
<p>ä¸€æ–‡äº†è§£é¢„è®­ç»ƒæ¨¡å‹ Prompt å¾®è°ƒï¼ˆæ¯”è¾ƒè¯¦ç»†ï¼‰</p>
<p><a href="https://zhuanlan.zhihu.com/p/572970562">https://zhuanlan.zhihu.com/p/572970562</a></p>
</li>
</ul>
<h2 id="5-æ–‡æœ¬ç”Ÿæˆ">5 æ–‡æœ¬ç”Ÿæˆ<a hidden class="anchor" aria-hidden="true" href="#5-æ–‡æœ¬ç”Ÿæˆ">#</a></h2>
<ul>
<li>
<p>Evaluation of Text Generation: A Survey [2020]</p>
<p><a href="https://arxiv.org/pdf/2006.14799.pdf">https://arxiv.org/pdf/2006.14799.pdf</a></p>
</li>
<li>
<p>A Survey of Evaluation Metrics Used for NLG Systems [2020]</p>
<p><a href="http://export.arxiv.org/pdf/2008.12009v2.pdf">http://export.arxiv.org/pdf/2008.12009v2.pdf</a></p>
</li>
</ul>
<h2 id="6-ç”Ÿæˆå¼å¯¹è¯">6 ç”Ÿæˆå¼å¯¹è¯<a hidden class="anchor" aria-hidden="true" href="#6-ç”Ÿæˆå¼å¯¹è¯">#</a></h2>
<ul>
<li>
<p>å¼€æºæ¨¡å‹åŠä»£ç 
T5åœ¨ä¼šè¯é¢†åŸŸçš„åº”ç”¨
<a href="https://huggingface.co/microsoft/GODEL-v1_1-large-seq2seq">https://huggingface.co/microsoft/GODEL-v1_1-large-seq2seq</a>
<a href="https://huggingface.co/openbmb/cpm-ant-10b/tree/main">https://huggingface.co/openbmb/cpm-ant-10b/tree/main</a></p>
<p><a href="https://huggingface.co/TsinghuaAI/CPM-Generate">https://huggingface.co/TsinghuaAI/CPM-Generate</a></p>
</li>
</ul>
<h2 id="7-åŸºäºinstructçš„å¤šä»»åŠ¡å­¦ä¹ ">7 åŸºäºinstructçš„å¤šä»»åŠ¡å­¦ä¹ <a hidden class="anchor" aria-hidden="true" href="#7-åŸºäºinstructçš„å¤šä»»åŠ¡å­¦ä¹ ">#</a></h2>
<ul>
<li>
<p>ä»£è¡¨è®ºæ–‡</p>
</li>
<li>
<p><strong>Cross-Task Generalization via Natural Language Crowdsourcing Instructions</strong>
Cross-task generalization via natural language crowdsourcing instructions
Super-NaturalInstructions:Generalization via Declarative Instructions on 1600+ Tasks</p>
<p><a href="https://github.com/allenai/natural-instructions">https://github.com/allenai/natural-instructions</a></p>
<p><a href="https://instructions.apps.allenai.org/">https://instructions.apps.allenai.org/</a></p>
</li>
<li>
<p>FLAN:FINETUNED LANGUAGE MODELS ARE ZERO-SHOTLEARNERS
<a href="http://export.arxiv.org/pdf/2109.01652v5.pdf">http://export.arxiv.org/pdf/2109.01652v5.pdf</a>
FLANï¼šå¾®è°ƒè¯­è¨€æ¨¡å‹æ˜¯Zero-Shotå­¦ä¹ å™¨
<a href="http://www.syrr.cn/news/7832.html">http://www.syrr.cn/news/7832.html</a>
è°·æ­ŒFLAN-T5ä½œè€…äº²è®²ï¼š5400äº¿å‚æ•°ï¼Œ1800ä¸ªä»»åŠ¡ï¼Œå¦‚ä½•å®ç°å¤§è¯­è¨€æ¨¡å‹â€œè‡ªæˆ‘æ”¹è¿›â€
<a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247554093&amp;idx=2&amp;sn=a179a28a66be7956542d42bfe49ff2ee&amp;chksm=ebb72cf9dcc0a5ef3dc7104ec3bc949ac3e288177c38fd161bfaf03ee2b867d9f67ef38786b3&amp;scene=27">åœ°å€</a></p>
</li>
<li>
<p><strong>chatGPTç›¸å…³</strong></p>
</li>
<li>
<p><strong>ï¼ˆé‡è¦ï¼‰Survey for In-context Learning</strong></p>
<p><a href="https://arxiv.org/pdf/2301.00234v1.pdf">https://arxiv.org/pdf/2301.00234v1.pdf</a></p>
<p>WebGPT: Browser-assisted question-answering with human feedback</p>
<p><a href="https://arxiv.org/abs/2112.09332">https://arxiv.org/abs/2112.09332</a></p>
<p>Lessons Learned on Language Model Safety and Misuse</p>
<p><a href="https://openai.com/blog/language-model-safety-and-misuse/">https://openai.com/blog/language-model-safety-and-misuse/</a></p>
<p>structGPT:Scaling Laws for Reward Model Overoptimization</p>
<p><a href="https://arxiv.org/abs/2210.10760">https://arxiv.org/abs/2210.10760</a></p>
<p>Learning to summarize with human feedback</p>
<p><a href="https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html</a></p>
</li>
</ul>
<h2 id="8-chatgptçš„prompts">8 chatGPTçš„prompts<a hidden class="anchor" aria-hidden="true" href="#8-chatgptçš„prompts">#</a></h2>
<ul>
<li>
<p>chatGPTçš„è‹±æ–‡prompt
<a href="https://github.com/f/awesome-chatgpt-prompts">https://github.com/f/awesome-chatgpt-prompts</a></p>
</li>
<li>
<p>chatGPTçš„ä¸­æ–‡prompt</p>
<p><a href="https://github.com/PlexPt/awesome-chatgpt-prompts-zh">https://github.com/PlexPt/awesome-chatgpt-prompts-zh</a></p>
</li>
<li>
<p>chatGPTæŠ€èƒ½æ¢ç´¢</p>
</li>
</ul>
<h2 id="9-é¢„è®­ç»ƒæ•°æ®">9 é¢„è®­ç»ƒæ•°æ®<a hidden class="anchor" aria-hidden="true" href="#9-é¢„è®­ç»ƒæ•°æ®">#</a></h2>
<ul>
<li>
<p>The Pile: An 800GB Dataset of Diverse Text for Language Modeling
<a href="https://arxiv.org/abs/2101.00027">https://arxiv.org/abs/2101.00027</a></p>
</li>
<li>
<p>ChatGPT æ•°æ®é›†ä¹‹è°œ
<a href="https://mp.weixin.qq.com/s?__biz=MzA5ODEzMjIyMA==&amp;mid=2247700799&amp;idx=2&amp;sn=1e3ed03d7c7c637775bec28253c2c586&amp;chksm=909bd8aca7ec51ba28041f7c1e694a3df2ac78038fca80dc8573d192c849f51099187b4cf48a&amp;mpshare=1&amp;scene=1&amp;srcid=0215lXttfVBw7Qh9TAlrJcJK&amp;sharer_sharetime=1676511276776&amp;sharer_shareid=95e17c8e615358fda71a20aabf4db300&amp;exportkey=n_ChQIAhIQUHFcRtmxCAxN%2Funs10MVjBKZAgIE97dBBAEAAAAAAGcQKvAIhioAAAAOpnltbLcz9gKNyK89dVj0StQ9%2F4SqWBIe%2BAbTr%2BUGK88lpPyfaOiy48kOlyMcaHXQQDGFqB8alNeXQ6oLO40GtQw3Ivthm%2FI7gGR1eGXrjYzqX4%2FqwnHXnjPIc%2BfRA0F%2BMSIPltfTCqDClD2cFdFVKMSiVib0dqUL6Ltr3nPhsCieYWiWilVt94vnPXT74SxsA2tDC%2BNfYKwS17qMvE1BZgK9p6iGC1gA2eX00r44nPE8niedVqZTgXGu1CixHp30MoNcQB5XkKGYZW2o6tTwVa2fMa8%2BqYBiGYBOD2XltPQtssw6RLupkN%2Fu53q3lnojbIdTimg5%2FZThQcPuzCVmrj21&amp;acctmode=0&amp;pass_ticket=O8m86P11g6%2BGBbI2SuOjF5sm8UiWk7FN3bJyc7bze6djd8ECLDlPhbPfxZxo6IPRNkmd9JXZg41DKU9aYmU4cw%3D%3D&amp;wx_header=0#rd">åœ°å€</a></p>
</li>
</ul>
<h2 id="10-è®­ç»ƒæˆæœ¬åˆ†æ">10 è®­ç»ƒæˆæœ¬åˆ†æ<a hidden class="anchor" aria-hidden="true" href="#10-è®­ç»ƒæˆæœ¬åˆ†æ">#</a></h2>
<ul>
<li>
<p>æ ·ä¾‹1 å†»ç»“éƒ¨åˆ†å‚æ•°é¢„è®­ç»ƒbig model
<a href="https://huggingface.co/IDEA-CCNL/Randeng-T5-784M">https://huggingface.co/IDEA-CCNL/Randeng-T5-784M</a></p>
<p>æˆ‘ä»¬åŸºäºmT5-largeï¼Œè®­ç»ƒäº†å®ƒçš„ä¸­æ–‡ç‰ˆã€‚ä¸ºäº†åŠ é€Ÿè®­ç»ƒï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨T5åˆ†è¯å™¨(sentence piece)ä¸­çš„ä¸­è‹±æ–‡å¯¹åº”çš„è¯è¡¨ï¼Œå¹¶ä¸”ä½¿ç”¨äº†è¯­æ–™åº“è‡ªé€‚åº”é¢„è®­ç»ƒ(Corpus-Adaptive Pre-Training, CAPT)æŠ€æœ¯åœ¨æ‚Ÿé“è¯­æ–™åº“(180Gç‰ˆæœ¬)ç»§ç»­é¢„è®­ç»ƒã€‚é¢„è®­ç»ƒç›®æ ‡ä¸ºç ´åspanã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒé˜¶æ®µä¸­ä½¿ç”¨äº†<a href="https://github.com/IDEA-CCNL/Fengshenbang-LM/tree/main/fengshen">å°ç¥æ¡†æ¶</a>å¤§æ¦‚èŠ±è´¹äº†16å¼ A100çº¦96å°æ—¶ã€‚</p>
<p>ä½¿ç”¨è¯„ä¼°ç½‘ç«™ï¼šhttps://www.autodl.com/home</p>
<p>ä¸Šè¿°è®­ç»ƒèŠ±è´¹ä»£ä»·(é‡‡ç”¨16å¼ <code>A100</code>è®­ç»ƒ<code>180G</code>è¯­æ–™-ä»…ä»…è®­ç»ƒäº†è¯è¡¨çš„embeddingï¼Œè¯¦è§<a href="https://arxiv.org/pdf/2209.02970.pdf">è®ºæ–‡</a>)ï¼š7.98å…ƒ/h x 16 x 96=12257.28å…ƒ</p>
</li>
</ul>
<p><img loading="lazy" src="image-20230210140512616.png" alt="image-20230210140512616"  />
</p>
<ul>
<li>æ ·ä¾‹2 å…¨éƒ¨å‚æ•°é¢„è®­ç»ƒ big model
<a href="https://www.zhiu.cn/156272.html">https://www.zhiu.cn/156272.html</a></li>
</ul>
<h2 id="11-å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶">11 å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶<a hidden class="anchor" aria-hidden="true" href="#11-å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶">#</a></h2>
<ul>
<li>DeepSpeed
<a href="https://www.deepspeed.ai/">https://www.deepspeed.ai/</a></li>
<li>ColossalAI
<a href="https://github.com/hpcaitech/ColossalAI">https://github.com/hpcaitech/ColossalAI</a></li>
</ul>
<h2 id="12-é’ˆå¯¹chatgptçš„è®­ç»ƒè¿‡ç¨‹å¤ç°">12 é’ˆå¯¹chatGPTçš„è®­ç»ƒè¿‡ç¨‹å¤ç°<a hidden class="anchor" aria-hidden="true" href="#12-é’ˆå¯¹chatgptçš„è®­ç»ƒè¿‡ç¨‹å¤ç°">#</a></h2>
<ul>
<li>
<p><strong>æ ·ä¾‹</strong></p>
</li>
<li>
<p>colossal-ai-chatgpt
Open source solution replicates ChatGPT training process! Ready to go with only 1.6GB GPU memory and gives you 7.73 times faster training!</p>
<p><a href="https://www.hpc-ai.tech/blog/colossal-ai-chatgpt">https://www.hpc-ai.tech/blog/colossal-ai-chatgpt</a></p>
</li>
<li>
<p>åŸºäºæƒ…æ„Ÿåˆ†ç±»çš„ç±»chatGPTè®­ç»ƒ</p>
<p>å®Œæ•´æºç åœ¨è¿™é‡Œï¼š</p>
<p><em><a href="https://github.com/HarderThenHarder/transformers_tasks/tree/main/RLHF">https://github.com/HarderThenHarder/transformers_tasks/tree/main/RLHF</a></em></p>
</li>
<li>
<p><strong>é‡è¦æ€è·¯</strong></p>
</li>
<li>
<p>å®Œå…¨ä»é›¶å®ç°chatGPT-<a href="https://karpathy.ai/">Andrej Karpathy</a>
Bç«™æ¬è¿
<a href="https://space.bilibili.com/3129054/channel/collectiondetail?sid=874339">https://space.bilibili.com/3129054/channel/collectiondetail?sid=874339</a>
è¯¾ç¨‹ä¸»é¡µ:https://github.com/karpathy/nn-zero-to-hero
é‡ç‚¹é¡¹ç›®::https://github.com/karpathy/nanoGPT</p>
</li>
<li>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›åˆ†æä¸åº”ç”¨ã€€é‚±é”¡é¹å¤æ—¦å¤§å­¦
<a href="https://www.bilibili.com/video/BV1Tx4y1w78p">https://www.bilibili.com/video/BV1Tx4y1w78p</a></p>
</li>
</ul>
<h2 id="13-å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»‹ç»">13 å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»‹ç»<a hidden class="anchor" aria-hidden="true" href="#13-å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»‹ç»">#</a></h2>
<ul>
<li>Illustrating Reinforcement Learning from Human Feedback (RLHF)
<a href="https://huggingface.co/blog/rlhf">https://huggingface.co/blog/rlhf</a></li>
</ul>
<h2 id="14-chatgptæŠ€æœ¯æ€è€ƒ">14 chatGPTæŠ€æœ¯æ€è€ƒ<a hidden class="anchor" aria-hidden="true" href="#14-chatgptæŠ€æœ¯æ€è€ƒ">#</a></h2>
<ul>
<li>
<p>ChatGPT èƒŒåçš„â€œåŠŸè‡£â€â€”â€”RLHF æŠ€æœ¯è¯¦è§£
<a href="https://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247484347&amp;idx=1&amp;sn=216b180e33cd4a422e3027c8176893cd&amp;scene=21#wechat_redirect">åœ°å€</a>
è§£è¯» ChatGPT èƒŒåçš„æŠ€æœ¯é‡ç‚¹ï¼šRLHFã€IFTã€CoTã€çº¢è“å¯¹æŠ—
<a href="https://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&amp;mid=2449950138&amp;idx=1&amp;sn=ed1b43743bc6cdf3ecf3fe05b1afc565&amp;chksm=b13c43d9864bcacf262ed0c2540d1dab3aef587ea7c56c35528356cc94293ce8bb3573b3f208&amp;mpshare=1&amp;scene=1&amp;srcid=0216TtZLE0MPIQrn3zzOzP3Z&amp;sharer_sharetime=1676505971091&amp;sharer_shareid=95e17c8e615358fda71a20aabf4db300&amp;exportkey=n_ChQIAhIQomH%2FyfMIdC6dMaAfFjvD6RKZAgIE97dBBAEAAAAAADBcNANJlJoAAAAOpnltbLcz9gKNyK89dVj04Fi%2BN243YM0QctmSL7Y2jJtyRzp3uvXzOjoncQL6jFD88yr6xUbLgaVJRWWwtqzJnN%2BUQaydoDDYRRNPtBO1QGSsJzHCOCh6VwoHinCvnfMEQsXpcGCafI%2F8kR8sNBrBHp8o4oiwPm2MkwvMG1uNajwoXpWGLzVRpEdYDYZnHwFH9M4UDHjiTFb7FrqIihc3U0A6%2F8JWuJhupGEB98p%2F%2BXyJnx8l6y0IL9y%2FdJKK8lJdOvwlA3ib1rF%2B66U2X4Ru9Z2ZRj610ZFgQygWtjXq1Tk%2B3xxgNmtcmfDeOOotLl0yNSeihsno9jc%2FQad8fTnv5JbO&amp;acctmode=0&amp;pass_ticket=O8m86P11g6%2BGBbI2SuOjF5sm8UiWk7FN3bJyc7bze6dR3kWW3iO77GbHWnJDqirQ35EqKPUAsIswSvz5VVpPfA%3D%3D&amp;wx_header=0#rd">åœ°å€</a></p>
</li>
<li>
<p><strong>(é‡è¦)What Makes a Dialog Agent Useful?</strong>
<a href="https://huggingface.co/blog/dialog-agents">https://huggingface.co/blog/dialog-agents</a></p>
</li>
<li>
<p>çº¢è“å¯¹æŠ— (red-teaming)
<a href="https://arxiv.org/abs/2209.07858">https://arxiv.org/abs/2209.07858</a></p>
</li>
<li>
<p>å¼ æ ‹ï¼šChatGPT åˆ¶èƒœå…¬å¼
<a href="https://hub.baai.ac.cn/view/24166">https://hub.baai.ac.cn/view/24166</a></p>
</li>
<li>
<p><strong>(é‡è¦)How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources</strong></p>
<p><a href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1">https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1</a></p>
</li>
<li>
<p><strong>(é‡è¦)Chain of Thought Prompting Elicits Reasoning in Large Language Models</strong></p>
<p><a href="https://arxiv.org/pdf/2201.11903v1.pdf">https://arxiv.org/pdf/2201.11903v1.pdf</a></p>
</li>
<li>
<p>æ·±å…¥ç†è§£è¯­è¨€æ¨¡å‹çš„çªç°èƒ½åŠ›</p>
<p><a href="https://yaofu.notion.site/514f4e63918749398a1a8a4c660e0d5b">https://yaofu.notion.site/514f4e63918749398a1a8a4c660e0d5b</a></p>
</li>
</ul>
<h2 id="15-chatgptå½±å“">15 chatGPTå½±å“<a hidden class="anchor" aria-hidden="true" href="#15-chatgptå½±å“">#</a></h2>
<ul>
<li>
<p>è”æƒ³CTOèŠ®å‹‡ï¼šAIå¤§æ¨¡å‹ä¸ºæ™ºèƒ½åŒ–å˜é©å¸¦æ¥çš„æœºé‡å’ŒæŒ‘æˆ˜</p>
<p><a href="https://hub.baai.ac.cn/view/24171">https://hub.baai.ac.cn/view/24171</a></p>
</li>
<li>
<p>Reid Hoffmanï½œç›¸ä¿¡è‡ªå·±ï¼Œå…¶å®ä½ æ¯”æƒ³è±¡ä¸­æ›´æœ‰å‡†å¤‡</p>
</li>
<li>
<p>ä¸­é‡‘ | AIåå¹´å±•æœ›ï¼ˆäº”ï¼‰ï¼šä»ChatGPTåˆ°é€šç”¨æ™ºèƒ½ï¼Œæ–°é•¿å¾ä¸Šçš„æ–°å˜åŒ–
<a href="https://mp.weixin.qq.com/s?__biz=MzI3MDMzMjg0MA==&amp;mid=2247614435&amp;idx=4&amp;sn=f8fa5c51c13fd56eaa976901d1f182e2&amp;chksm=ead1d924dda65032d9c518fece7a973236ba2c7011e35e7c214c9f844a7c09ecc5d685210c43&amp;mpshare=1&amp;scene=1&amp;srcid=0203K8jtHijzZr6wvULIWERm&amp;sharer_sharetime=1675402722773&amp;sharer_shareid=18016bb3ee6115f00a7e5625b81a673e&amp;exportkey=n_ChQIAhIQb3FDsA9uidul3FVjuQWEAxKZAgIE97dBBAEAAAAAAGuIBYfEDTIAAAAOpnltbLcz9gKNyK89dVj0AYsV55gisAOmbR5so5YDwwWZC4p8JH5WABUPyxt7qU2jxpLh7QbuG0BfZT7kMtin0hWmFahxn%2BTXqMtXLp5vK1Xw0wp%2FlzYKjFcZQ91fPmq5gvHT8K%2BgVUmR6wrraAL0UtfxAG5fTCWzTG7%2B7hHnySmkdoa8JivIIsaIyA9naSG9pU%2FvL14d7cKszMDuLEl%2BUmKWcxOyb4nTx8OaGEkPVVgceQQcb0FfXP2eBtGAKBwERr9wh3aRyD8jSnAkZ4841zt8xXjFNPGov1mXrNwR7CYlSmNhNDKV6jRM1VVZCYcbXDOHWm6bU%2Bf3M584Ph4Z82GP&amp;acctmode=0&amp;pass_ticket=6H5e0SHvuq7CJmsgWvzRLZHdowuOKCxpdjNZVSUjSqlMUfveOL24taTc04b5gigr3zEDtmKj7UgCZlvF%2BIF6BA%3D%3D&amp;wx_header=0#rd">åœ°å€</a></p>
</li>
<li>
<p>AIGCï¼šChatGPT(ä¸€ä¸ªé‡Œç¨‹ç¢‘å¼çš„å¯¹è¯èŠå¤©æœºå™¨äºº)çš„ç®€ä»‹(æ„ä¹‰/åŠŸèƒ½/æ ¸å¿ƒæŠ€æœ¯ç­‰)ã€ä½¿ç”¨æ–¹æ³•(ä¸ƒç±»ä»»åŠ¡)ã€æ¡ˆä¾‹åº”ç”¨(æé—®åŸºç¡€æ€§/äº‹å®æ€§/é€»è¾‘æ€§/åˆ›é€ æ€§/å¼€æ”¾æ€§çš„é—®é¢˜ä»¥åŠç¼–ç¨‹ç›¸å…³)ä¹‹è¯¦ç»†æ”»ç•¥
<a href="https://yunyaniu.blog.csdn.net/article/details/128229941">åœ°å€</a></p>
</li>
<li>
<p>äººå·¥æ™ºèƒ½è¡Œä¸šChatGPTä¸“é¢˜ç ”ç©¶ï¼šå¼€å¯AIæ–°çºªå…ƒ.pdf
<a href="https://www.vzkoo.com/document/2023020328d4913255f87e4c853de0b8.html">https://www.vzkoo.com/document/2023020328d4913255f87e4c853de0b8.html</a></p>
<p><a href="https://hub.baai.ac.cn/view/24173">https://hub.baai.ac.cn/view/24173</a></p>
</li>
</ul>
<h2 id="16-attentionç›¸å…³">16 Attentionç›¸å…³<a hidden class="anchor" aria-hidden="true" href="#16-attentionç›¸å…³">#</a></h2>
<ul>
<li>
<p>æµ‹è¯•ä¸¤ç§æ–° Attention æœºåˆ¶ï¼šgMLP å’Œ AFTï¼ˆç»“è®ºï¼šAFT æ•ˆæœå¥½ï¼‰
<a href="https://zhuanlan.zhihu.com/p/395005917?utm_id=0">https://zhuanlan.zhihu.com/p/395005917?utm_id=0</a></p>
</li>
<li>
<p>RWKV is inspired by Apple&rsquo;s AFT (<a href="https://arxiv.org/abs/2105.14103)">https://arxiv.org/abs/2105.14103)</a>.</p>
<p>Moreover it&rsquo;s using a number of my tricks, such as:</p>
<ul>
<li>SmallInitEmb: <a href="https://github.com/BlinkDL/SmallInitEmb">https://github.com/BlinkDL/SmallInitEmb</a> (applicable to all transformers) which helps the embedding quality, and stabilizes Post-LN (which is what I am using).</li>
<li>Token-shift: <a href="https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing">https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing</a> (applicable to all transformers), especially helpful for char-level models.</li>
<li>Head-QK: <a href="https://github.com/BlinkDL/RWKV-LM#the-head-qk-trick-learning-to-copy-and-avoid-tokens">https://github.com/BlinkDL/RWKV-LM#the-head-qk-trick-learning-to-copy-and-avoid-tokens</a> (applicable to all transformers). Note: it&rsquo;s helpful, but I disabled it in the Pile model to keep it 100% RNN.</li>
<li>Extra R-gate in the FFN (applicable to all transformers). I am also using reluSquared from Primer.</li>
<li>Better initilization: I init most of the matrices to ZERO (see RWKV_Init in <a href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v2-RNN/src/model.py)">https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v2-RNN/src/model.py)</a>.</li>
<li>You can transfer some parameters from a small model to a large model (note: I sort &amp; smooth them too), for faster and better convergence (see <a href="https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/)">https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/)</a>.</li>
<li>My CUDA kernel: <a href="https://github.com/BlinkDL/RWKV-CUDA">https://github.com/BlinkDL/RWKV-CUDA</a> to speedup training.</li>
</ul>
</li>
</ul>
<h2 id="17-lmaasæ€ç»´é“¾">17 LMaaSæ€ç»´é“¾<a hidden class="anchor" aria-hidden="true" href="#17-lmaasæ€ç»´é“¾">#</a></h2>
<ul>
<li><strong>è®ºæ–‡æ¦‚è¿°æˆ–é›†åˆ</strong></li>
<li><strong>Towards Reasoning in Large Language Models: A Survey.</strong></li>
<li><a href="https://arxiv.org/abs/2212.10403">https://arxiv.org/abs/2212.10403</a></li>
<li>A trend starts from &ldquo;Chain of Thought Prompting Elicits Reasoning in Large Language Models&rdquo;.</li>
<li><a href="https://github.com/Timothyxxx/Chain-of-ThoughtsPapers">https://github.com/Timothyxxx/Chain-of-ThoughtsPapers</a></li>
<li><strong>ä»£è¡¨è®ºæ–‡</strong></li>
<li>æ€ç»´é“¾æç¤º (Wei ç­‰, &lsquo;22):
<a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a></li>
<li>Letâ€™s think step by step:
<a href="https://arxiv.org/abs/2205.11916">https://arxiv.org/abs/2205.11916</a></li>
<li>CoT å›¾è§£ç¤ºä¾‹ (Chung ç­‰, &lsquo;22):
<a href="https://arxiv.org/abs/2210.11416">https://arxiv.org/abs/2210.11416</a></li>
<li>CoT å¾®è°ƒä¹Ÿæ˜¾ç¤ºå‡ºå¯¹æ— å®³æ€§éå¸¸æœ‰æ•ˆ (Bai ç­‰, &lsquo;22):
<a href="https://www.anthropic.com/constitutional.pdf">https://www.anthropic.com/constitutional.pdf</a></li>
<li><strong>Large Language Models Are Reasoning Teachers.</strong></li>
<li><a href="https://arxiv.org/abs/2212.10071">https://arxiv.org/abs/2212.10071</a></li>
<li><strong>å¼€æ”¾æ¨¡å‹åŠä»£ç </strong></li>
<li>Automatic Chain of Thought Prompting in Large Language Models
<a href="https://github.com/amazon-science/auto-cot">https://github.com/amazon-science/auto-cot</a></li>
</ul>
<h2 id="18--lmaaså®‰å…¨æ€§">18  LMaaSå®‰å…¨æ€§<a hidden class="anchor" aria-hidden="true" href="#18--lmaaså®‰å…¨æ€§">#</a></h2>
<ul>
<li>Unnatural Instructions (Honovich ç­‰, &lsquo;22):
<a href="https://arxiv.org/abs/2212.09689">https://arxiv.org/abs/2212.09689</a></li>
<li>Super-natural instructions (Wang ç­‰, &lsquo;22):
<a href="https://arxiv.org/abs/2204.07705">https://arxiv.org/abs/2204.07705</a></li>
<li>Self-Instruct (Wang ç­‰, &lsquo;22):
<a href="https://arxiv.org/abs/2212.10560">https://arxiv.org/abs/2212.10560</a></li>
<li>T0 (Sanh ç­‰, &lsquo;22):
<a href="https://arxiv.org/abs/2110.08207">https://arxiv.org/abs/2110.08207</a></li>
<li>Natural instructions æ•°æ®é›† (Mishra ç­‰, &lsquo;22):
<a href="https://arxiv.org/abs/2104.08773">https://arxiv.org/abs/2104.08773</a></li>
<li>FLAN LM (Wei ç­‰, &lsquo;22):
<a href="https://arxiv.org/abs/2109.01652">https://arxiv.org/abs/2109.01652</a></li>
<li>OPT-IML (Iyer ç­‰, &lsquo;22):
<a href="https://arxiv.org/abs/2212.12017">https://arxiv.org/abs/2212.12017</a></li>
</ul>
<h2 id="19-å¯¹è¯æœºå™¨äººäº§å“">19 å¯¹è¯æœºå™¨äººäº§å“<a hidden class="anchor" aria-hidden="true" href="#19-å¯¹è¯æœºå™¨äººäº§å“">#</a></h2>
<ul>
<li>Meta çš„ BlenderBot:
<a href="https://arxiv.org/abs/2208.03188">https://arxiv.org/abs/2208.03188</a></li>
<li>Google çš„ LaMDA:
<a href="https://arxiv.org/abs/2201.08239">https://arxiv.org/abs/2201.08239</a></li>
<li>DeepMind çš„ Sparrow:
<a href="https://arxiv.org/abs/2209.14375">https://arxiv.org/abs/2209.14375</a></li>
<li>Anthropic çš„ Assistant:
<a href="https://arxiv.org/abs/2204.05862">https://arxiv.org/abs/2204.05862</a></li>
</ul>
<h2 id="20-å¯¹è¯æ•°æ®é›†">20 å¯¹è¯æ•°æ®é›†<a hidden class="anchor" aria-hidden="true" href="#20-å¯¹è¯æ•°æ®é›†">#</a></h2>
<ul>
<li>ä¸‰ä¸ªæ•°æ®é›†ï¼š</li>
</ul>
<p>ï¼ˆï¼‘ï¼‰é—²èŠã€€</p>
<p>ï¼ˆï¼’ï¼‰åŸºäºæ–‡æ¡£çš„é—®é¢˜å›å¤ã€€Dureader
<strong>Dureader*checklist*é˜…è¯»ç†è§£ç»†ç²’åº¦è¯„ä¼°æ•°æ®é›†</strong>&ndash;ä¸å¯èƒ½çš„ç›´æ¥ç»™æ–‡æ¡£</p>
<p>â€‹	<strong>DuReader*robust*é˜…è¯»ç†è§£é²æ£’æ€§æ•°æ®é›†</strong></p>
<p>ï¼ˆï¼“ï¼‰å¤šè½®ä¼šè¯è¯­æ–™</p>
<p>â€‹	<strong>DuLeMonä¸­æ–‡é•¿æ—¶è®°å¿†å¯¹è¯æ•°æ®é›†</strong></p>
<p>â€‹	<strong>Diamanteä¸­æ–‡å¼€æ”¾åŸŸé—²èŠæ•°æ®é›†</strong></p>
<p>â€‹	<strong>DuSincæœåŠ¡ä¿¡æ¯å¢å¼ºå¯¹è¯æ•°æ®é›†</strong></p>
<p>â€‹	CrossWOZ</p>
<p>â€‹	https://github.com/thu-coai/<em>CrossWOZ</em></p>
<h2 id="21-å…¶ä»–äººæ€»ç»“chatgpt">21 å…¶ä»–äººæ€»ç»“chatGPT<a hidden class="anchor" aria-hidden="true" href="#21-å…¶ä»–äººæ€»ç»“chatgpt">#</a></h2>
<ul>
<li>å…³äºChatGPTçš„é¢„è®­ç»ƒå’Œè°ƒä¼˜æ–¹æ³•çš„å¿…è¯»è®ºæ–‡ã€ç›¸å…³åšå®¢å’ŒAPIå·¥å…·ã€‚
<a href="https://chatgpt.pro/">https://chatgpt.pro/</a>
<a href="https://github.com/shizhediao/ChatGPTPapers">https://github.com/shizhediao/ChatGPTPapers</a></li>
<li><strong>å·²å£°æ˜ç±»chatGPTå†…æµ‹é˜¶æ®µçš„æœºæ„æˆ–å…¬å¸</strong>
<ul>
<li>äº¬ä¸œ</li>
<li>ç™¾åº¦</li>
<li>é˜¿é‡Œ</li>
<li>è…¾è®¯</li>
<li>chatYuan</li>
<li>å¤æ—¦</li>
</ul>
</li>
</ul>
<h2 id="22-å¤šæŠ€èƒ½å¯¹è¯">22 å¤šæŠ€èƒ½å¯¹è¯<a hidden class="anchor" aria-hidden="true" href="#22-å¤šæŠ€èƒ½å¯¹è¯">#</a></h2>
<ul>
<li>
<p><strong>ä»»åŠ¡å‹å¯¹è¯</strong></p>
</li>
<li>
<p>A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation</p>
</li>
<li>
<p>Estimating Soft Labels for Out-of-Domain Intent Detection</p>
</li>
<li>
<p><strong>è¡¨æ ¼å‹å¯¹è¯</strong></p>
</li>
<li>
<p>STAR: SQL Guided Pre-Training for Context-dependent Text-to-SQL Parsing</p>
</li>
<li>
<p>Towards Generalizable and Robust Text-to-SQL Parsing</p>
</li>
<li>
<p><strong>æ–‡æ¡£å‹å¯¹è¯</strong></p>
</li>
<li>
<p>Towards Generalized Open Information Extraction</p>
</li>
<li>
<p>Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots</p>
</li>
<li>
<p><strong>å¤šæ¨¡æ€å¯¹è¯</strong></p>
</li>
<li>
<p><strong>å¯¹è¯ç³»ç»Ÿçš„ç»ˆèº«å­¦ä¹ </strong></p>
</li>
<li>
<p>Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue</p>
</li>
<li>
<p>Semi-Supervised Lifelong Language Learning</p>
</li>
</ul>
<h2 id="23-çŸ¥è¯†å‹å¯¹è¯">23 çŸ¥è¯†å‹å¯¹è¯<a hidden class="anchor" aria-hidden="true" href="#23-çŸ¥è¯†å‹å¯¹è¯">#</a></h2>
<p>åŠ¨æ€çŸ¥è¯†å¯¹è¯</p>
<ul>
<li>SINC: Service Information Augmented Open-Domain Conversation
åˆå:Link the World: Improving Open-domain Conversation with Dynamic Spatiotemporal-aware Knowledge
<a href="https://arxiv.org/pdf/2206.14000v2.pdf">https://arxiv.org/pdf/2206.14000v2.pdf</a></li>
</ul>
<p>æ˜¾å¼ç›®æ ‡å¯¹è¯</p>
<ul>
<li>DuConv: Proactive Human-Machine Conversation with Explicit Conversation Goals
<a href="https://arxiv.org/pdf/1906.05572v2.pdf">https://arxiv.org/pdf/1906.05572v2.pdf</a></li>
</ul>
<p>çŸ¥è¯†å¯¹è¯</p>
<ul>
<li>
<p>KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation
<a href="https://arxiv.org/pdf/2004.04100v1.pdf">https://arxiv.org/pdf/2004.04100v1.pdf</a></p>
</li>
<li>
<p>KPT: Keyword-guided Pre-training for Grounded Dialog Generation</p>
</li>
</ul>
<h2 id="24-å¯¹è¯è¡¨ç¤ºå­¦ä¹ ">24 å¯¹è¯è¡¨ç¤ºå­¦ä¹ <a hidden class="anchor" aria-hidden="true" href="#24-å¯¹è¯è¡¨ç¤ºå­¦ä¹ ">#</a></h2>
<ul>
<li>
<p><strong>dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings</strong></p>
<p><a href="https://arxiv.org/abs/2210.15332v1">https://arxiv.org/abs/2210.15332v1</a></p>
<p><a href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial2vec">https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial2vec</a></p>
</li>
</ul>
<h2 id="25-å›½å†…å¤§å‚å¯¹è¯ç³»ç»Ÿå®éªŒå®¤">25 å›½å†…å¤§å‚å¯¹è¯ç³»ç»Ÿå®éªŒå®¤<a hidden class="anchor" aria-hidden="true" href="#25-å›½å†…å¤§å‚å¯¹è¯ç³»ç»Ÿå®éªŒå®¤">#</a></h2>
<h3 id="è¾¾æ‘©é™¢çš„space">è¾¾æ‘©é™¢çš„SPACE<a hidden class="anchor" aria-hidden="true" href="#è¾¾æ‘©é™¢çš„space">#</a></h3>
<ul>
<li>SPACE-1: <a href="https://arxiv.org/abs/2111.14592">https://arxiv.org/abs/2111.14592</a></li>
<li>SPACE-2: <a href="https://arxiv.org/abs/2209.06638">https://arxiv.org/abs/2209.06638</a></li>
<li>SPACE-3: <a href="https://arxiv.org/abs/2209.06664">https://arxiv.org/abs/2209.06664</a></li>
<li>ç›¸å…³ä»£ç ï¼šhttps://github.com/AlibabaResearch/DAMO-ConvAI</li>
</ul>
<h3 id="ç™¾åº¦çš„knover">ç™¾åº¦çš„Knover<a hidden class="anchor" aria-hidden="true" href="#ç™¾åº¦çš„knover">#</a></h3>
<ul>
<li>
<p>PLATO-1</p>
</li>
<li>
<p>PLATO-2</p>
</li>
<li>
<p>PLATO-XL</p>
</li>
<li>
<p>PLATO-KAG(<em>PLATO</em>-K)</p>
<ul>
<li>March 2022: We are opening <a href="https://github.com/PaddlePaddle/Knover/blob/ab547f0ba03c9142183d97c2ee6ed7a1c3750125/projects/PLATO-KAG/README.md">PLATO-KAG</a>, an unsupervised learning approach for end-to-end knowledge-grounded conversation modeling.</li>
<li>February 2022: We are opening our TOD-DA dataset, models and code in <a href="https://github.com/PaddlePaddle/Knover/blob/ab547f0ba03c9142183d97c2ee6ed7a1c3750125/projects/DSTC10-Track2/README.md">DSTC10-Track2</a>.</li>
<li>December 2021: We are opening the dialogue generation model of <a href="https://github.com/PaddlePaddle/Knover/blob/ab547f0ba03c9142183d97c2ee6ed7a1c3750125/projects/PLATO-XL/README.md">PLATO-XL</a>, with up to 11 billion parameters.</li>
<li>October 2021: We are opening <a href="https://github.com/PaddlePaddle/Knover/blob/ab547f0ba03c9142183d97c2ee6ed7a1c3750125/projects/AG-DST/README.md">AG-DST</a>, an amendable generation for dialogue state tracking.</li>
<li>February 2021: We are opening our implementation (Team 19) in <a href="https://github.com/PaddlePaddle/Knover/blob/ab547f0ba03c9142183d97c2ee6ed7a1c3750125/projects/DSTC9-Track1/README.md">DSTC9-Track1</a>.</li>
<li>July 2020: We are opening <a href="https://github.com/PaddlePaddle/Knover/blob/ab547f0ba03c9142183d97c2ee6ed7a1c3750125/projects/PLATO-2/README.md">PLATO-2</a>, a large-scale generative model with latent space for open-domain dialogue systems.</li>
</ul>
</li>
</ul>
<p>[1] <a href="arxiv.org/abs/2112.12441">TOD-DA: Towards Boosting the Robustness of Task-oriented Dialogue Modeling on Spoken Conversations</a></p>
<p>[2] <a href="arxiv.org/abs/2102.02096">Learning to Select External Knowledge with Multi-Scale Negative Sampling</a></p>
<p>[3] <a href="aclanthology.org/2021.nlp4convai-1.14">PLATO-KAG: Unsupervised Knowledge-Grounded Conversation via Joint Modeling</a></p>
<p>[4] <a href="arxiv.org/abs/2109.09519">PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation</a></p>
<p>[5] <a href="www.aclweb.org/anthology/2020.acl-main.9">PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable</a></p>
<h3 id="è…¾è®¯ai-lab-dialogue-research">è…¾è®¯AI lab Dialogue Research<a hidden class="anchor" aria-hidden="true" href="#è…¾è®¯ai-lab-dialogue-research">#</a></h3>
<p><a href="https://ai.tencent.com/ailab/nlp/dialogue/">https://ai.tencent.com/ailab/nlp/dialogue/</a></p>
<p>Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework</p>
<p><a href="https://aclanthology.org/D19-1195.pdf">https://aclanthology.org/D19-1195.pdf</a></p>
<p><a href="https://github.com/jcyk/seqgen">https://github.com/jcyk/seqgen</a></p>
<h2 id="26-å·¥ç¨‹åŒ–transformeråŠ é€Ÿ">26 (å·¥ç¨‹åŒ–)TransformeråŠ é€Ÿ<a hidden class="anchor" aria-hidden="true" href="#26-å·¥ç¨‹åŒ–transformeråŠ é€Ÿ">#</a></h2>
<ul>
<li>
<p>Efficient Transformers: A Survey</p>
<p><a href="http://export.arxiv.org/pdf/2009.06732v3.pdf">http://export.arxiv.org/pdf/2009.06732v3.pdf</a></p>
</li>
<li>
<p>Large Transformer Model Inference Optimization
<a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</a></p>
</li>
<li>
<p>The Transformer Family Version 2.0
<a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</a></p>
</li>
<li>
<p><strong>(ç•™å­˜)ChatGPT is not all you need. A State of the Art Review of large Generative AI models</strong></p>
<p>å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹</p>
</li>
<li>
<p>(ç•™å­˜)COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics</p>
<p>çº¦æŸæ¥æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„è¯­ä¹‰æˆ–æ ·å¼</p>
</li>
<li>
<p>(ç•™å­˜) Symmetry Teleportation for Accelerated Optimization
æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå‚æ•°ç©ºé—´å¯¹ç§°æ€§çš„ä¼˜åŒ–æ–¹æ³•ï¼ˆsymmetry teleportationï¼‰ï¼Œè¿™æ˜¯åœ¨åœ¨å‚æ•°ç©ºé—´ä¸Šä¿æŒæŸå¤±ä¸å˜çš„ä¸€ç»„åŠ¨ä½œï¼Œå®ƒå…è®¸å‚æ•°ç§»åŠ¨å¾ˆå¤§çš„è·ç¦»ï¼Œä»¥æé«˜åç»­æ­¥éª¤çš„æ”¶æ•›é€Ÿåº¦ã€ã€‚æœ¬æ–‡ç®—æ³•åˆ©ç”¨äº†é«˜é˜¶æ™¯è§‚å‡ ä½•ï¼Œä½†åœ¨å¤§å¤šæ•°æ­¥éª¤ä¸­åªä½¿ç”¨æ¢¯åº¦ä¿¡æ¯ï¼Œä»è€Œé¿å…äº†äºŒé˜¶æ–¹æ³•çš„è®¡ç®—æˆæœ¬ã€‚</p>
</li>
<li>
<p>(ç•™å­˜)Deep Bidirectional Language-Knowledge Graph Pretraining
çŸ¥è¯†å›¾è°±+è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒ
<a href="https://export.arxiv.org/pdf/2210.09338v2.pdf">https://export.arxiv.org/pdf/2210.09338v2.pdf</a></p>
</li>
<li>
<p>(ç•™å­˜) Are Pre-trained Convolutions Better than Pre-trained Transformers?
ä¸è¦å°†é¢„è®­ç»ƒçš„è¿›æ­¥ä¸æ¶æ„çš„è¿›æ­¥æ··ä¸ºä¸€è°ˆ</p>
</li>
<li>
<p>Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better</p>
</li>
<li>
<p><strong>(é‡è¦-ç²¾è¯») FastSeq: Make Sequence Generation Faster</strong>
The proposed optimization techniques include an <strong>attention cache optimization</strong>, an efficient algorithm for** detecting repeated n-grams**, and an **asynchronous
generation pipeline **with parallel I/O.
<a href="https://github.com/microsoft/fastseq">https://github.com/microsoft/fastseq</a></p>
</li>
<li>
<p>(ç•™å­˜) Query-driven Segment Selection for Ranking Long Documents</p>
</li>
<li>
<p><strong>(é‡è¦ï¼‰ Understanding and Overcoming the Challenges of Efficient Transformer Quantization.</strong>
<a href="https://github.com/qualcomm-ai-research/transformer-quantization">https://github.com/qualcomm-ai-research/transformer-quantization</a>.</p>
</li>
<li>
<p>Finetuning Pretrained Transformers into RNNs</p>
</li>
<li>
<p>A Primer on Pretrained Multilingual Language Models</p>
</li>
<li>
<p>(ç•™å­˜) AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing</p>
</li>
<li>
<p>EL-Attention: Memory Efficient Lossless Attention for Generation</p>
<p><strong>è‡ªå›å½’æ¨¡å‹è½¬éè‡ªå›å½’</strong></p>
</li>
<li>
<p>SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection and Slot Filling</p>
</li>
<li>
<p>Incorporating history and future into non-autoregressive machine translation</p>
</li>
<li>
<p>ï¼ˆç•™å­˜ï¼‰Non-Autoregressive Neural Machine Translation</p>
</li>
<li>
<p>An Effective Non-Autoregressive Model for Spoken Language Understanding</p>
</li>
<li>
<p>ï¼ˆé‡è¦ï¼‰A Study of Non-autoregressive Model for Sequence Generation</p>
</li>
<li>
<p>A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond</p>
</li>
<li>
<p>(é‡è¦)Directed Acyclic Transformer for Non-Autoregressive Machine Translation</p>
<p><a href="https://arxiv.org/abs/2205.07459">https://arxiv.org/abs/2205.07459</a></p>
</li>
<li>
<p>latent-GLAT: Glancing at Latent Variables for Parallel Text Generation</p>
<p><a href="https://github.com/baoy-nlp/latent-glat">https://github.com/baoy-nlp/latent-glat</a></p>
</li>
<li>
<p><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</a></p>
</li>
</ul>
<h2 id="27-ä½æ˜¾å­˜è¿è¡Œ">27 ä½æ˜¾å­˜è¿è¡Œ<a hidden class="anchor" aria-hidden="true" href="#27-ä½æ˜¾å­˜è¿è¡Œ">#</a></h2>
<ul>
<li>ChatRWKV v2 (wip) can split the model to gpu+cpu or gpu+gpu:
<a href="https://github.com/BlinkDL/ChatRWKV/tree/main/v2">https://github.com/BlinkDL/ChatRWKV/tree/main/v2</a>
(Stream mode soon)
è¿™ä¸ªå¯ä»¥æŠŠæ¨¡å‹åˆ†åˆ°åŒå¡ï¼Œæˆ–è€…ä¸€éƒ¨åˆ†GPUä¸€éƒ¨åˆ†CPUï¼ˆé€‚åˆåˆšå¥½æ”¾ä¸ä¸‹çš„æƒ…å†µï¼‰
ç¨åå†åŠ  STREAM æ¨¡å¼ï¼Œåœ¨å°æ˜¾å­˜GPUè·‘å¤§æ¨¡å‹ï¼ˆåŠ è½½å‡ å±‚æ¨¡å‹ï¼Œè·‘å‡ å±‚ï¼Œå†åŠ è½½å‡ å±‚ï¼Œè·‘å‡ å±‚ï¼‰</li>
</ul>
<h2 id="28-é—®é¢˜é‡å†™">28 é—®é¢˜é‡å†™<a hidden class="anchor" aria-hidden="true" href="#28-é—®é¢˜é‡å†™">#</a></h2>
<ul>
<li>
<p><strong>ï¼ˆé‡è¦) Question Rewriting for Conversational Question Answering (2021å¹´)</strong></p>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/3437963.3441748">https://dl.acm.org/doi/pdf/10.1145/3437963.3441748</a></p>
<p>å¯¹è¯å¼é—®é¢˜å›ç­”çš„é—®é¢˜é‡å†™</p>
</li>
<li>
<p>Reinforced Question Rewriting for Conversational Question Answering</p>
</li>
<li>
<p>Open-Domain Question Answering Goes Conversational via Question Rewriting</p>
<p><a href="https://github.com/akaysh/DenseQrecc">https://github.com/akaysh/DenseQrecc</a></p>
</li>
<li>
<p><a href="https://lrec2022.lrec-conf.org/en/">https://lrec2022.lrec-conf.org/en/</a>
<a href="https://github.com/Orange-OpenSource/COQAR">https://github.com/Orange-OpenSource/COQAR</a></p>
</li>
<li>
<p><strong>(é‡è¦)Improving Multi-turn Dialogue Modelling with Utterance ReWriter</strong></p>
<p><a href="https://github.com/liu-nlper/dialogue-utterance-rewriter.git">https://github.com/liu-nlper/dialogue-utterance-rewriter.git</a></p>
</li>
<li>
<p><strong>(é‡è¦-ç²¾è¯»)Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration (Pan et al., 2019)</strong></p>
<p><strong>Restoration-200K datasets</strong></p>
<p><a href="https://ai.tencent.com/ailab/nlp/dialogue/datasets/Restoration-200K.zip">https://ai.tencent.com/ailab/nlp/dialogue/datasets/Restoration-200K.zip</a></p>
</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>
<p>**(é‡è¦) Robust Dialogue Utterance Rewriting as Sequence Tagging **</p>
<p>é‡‡ç”¨æ ‡æ³¨&ndash;é€Ÿåº¦åŠ å¿«</p>
</li>
<li>
<p>Utterance Rewriting with Contrastive Learning in Multi-turn Dialogue</p>
</li>
<li>
<p><strong>(é‡è¦)SARG: A Novel Semi Autoregressive Generator for <em>Multi</em>-<em>turn</em> <em>Incomplete</em> <em>Utterance</em> <em>Restoration</em>.</strong></p>
<p><a href="https://github.com/NetEase-GameAI/SARG">https://github.com/NetEase-GameAI/SARG</a></p>
<p><strong>NAGå³Non-Autoregressive Model</strong></p>
</li>
<li>
<p><strong>å…¶ä»–</strong></p>
</li>
<li>
<p>æ–‡çŒ®ç»¼è¿°ä¹‹è¯­å¥é‡å†™, çœ‹è¿™ä¸€ç¯‡å°±å¤Ÿäº†!</p>
<p>â€‹	https://zhuanlan.zhihu.com/p/405209386</p>
</li>
</ul>
<h2 id="29-å¯¹è¯ç³»ç»Ÿsurvey">29 å¯¹è¯ç³»ç»Ÿsurvey<a hidden class="anchor" aria-hidden="true" href="#29-å¯¹è¯ç³»ç»Ÿsurvey">#</a></h2>
<ul>
<li>
<p>The AI Doctor Is In: A <em>Survey</em> of <em>Task</em>-<em>Oriented</em> Dialogue Systems for Healthcare Applications</p>
</li>
<li>
<p><strong>ï¼ˆé€‚åˆï¼‰Recent Neural Methods on Slot Filling and Intent Classification for Task-Oriented Dialogue Systems: A Survey.</strong></p>
</li>
<li>
<p>A Survey on Spoken Language Understanding: Recent Advances and New Frontiers</p>
</li>
<li>
<p>A Transformer based Multi-task Model for Domain Classification, Intent Detection and Slot-Filling</p>
</li>
<li>
<p><strong>Multi-Task Pre-Training for Plug-and-Play(å³ç”¨å³æ’) Task-Oriented Dialogue System</strong></p>
<p>å³æ’å³ç”¨å‹é¢å‘ä»»åŠ¡å¯¹è¯ç³»ç»Ÿçš„å¤šä»»åŠ¡é¢„è®­ç»ƒ&ndash;ç¬¬ä¸€æ¬¡è§è¿™ä¸ªææ³•ï¼ˆ<strong>è½¬å˜æˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤ä»»åŠ¡</strong>-é‡‡ç”¨T5å¤§æ¨¡å‹ï¼‰</p>
<p><a href="https://export.arxiv.org/pdf/2109.14739.pdf">https://export.arxiv.org/pdf/2109.14739.pdf</a></p>
<p><a href="https://github.com/awslabs/pptod">https://github.com/awslabs/pptod</a></p>
</li>
<li>
<p><strong>ï¼ˆé€‚åˆï¼‰Conversational Question Answering: A Survey.</strong></p>
</li>
<li>
<p>A survey: Conversational Knowledge Base Question Answering</p>
</li>
</ul>
<h2 id="30-ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿ">30 ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿ<a hidden class="anchor" aria-hidden="true" href="#30-ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿ">#</a></h2>
<ul>
<li>
<p>A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation Estimating Soft Labels for Out-of-Domain Intent Detection</p>
</li>
<li>
<p>A GuessWhat?! Game for Goal-Oriented Visual Dialog: A Survey</p>
</li>
<li>
<p>User Utterance Acquisition for Training Task-Oriented Bots: A Review of Challenges, Techniques and Opportunities</p>
</li>
<li>
<p>Estimating Soft Labels for Out-of-Domain Intent Detection</p>
</li>
<li>
<p><strong>(é‡è¦ï¼ç²¾è¯»)Multi-Task Pre-Training for Plug-and-Play(å³ç”¨å³æ’) Task-Oriented Dialogue System</strong></p>
<p><a href="https://export.arxiv.org/pdf/2109.14739.pdf">https://export.arxiv.org/pdf/2109.14739.pdf</a></p>
<p><a href="https://github.com/awslabs/pptod">https://github.com/awslabs/pptod</a></p>
<p>å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒ</p>
</li>
<li>
<p>A Unified Multi-task Learning Framework for Multi-goal Conversational Recommender Systems</p>
</li>
</ul>
<h2 id="31-åŸºäºä¸Šä¸‹æ–‡çš„å›å¤æ£€ç´¢">31 åŸºäºä¸Šä¸‹æ–‡çš„å›å¤æ£€ç´¢<a hidden class="anchor" aria-hidden="true" href="#31-åŸºäºä¸Šä¸‹æ–‡çš„å›å¤æ£€ç´¢">#</a></h2>
<p><strong>Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems</strong></p>
<p><a href="https://aichatbot.feishu.cn/file/boxcni2xhINHrIfnIkoEIUTWyGg">https://aichatbot.feishu.cn/file/boxcni2xhINHrIfnIkoEIUTWyGg</a></p>
<p>A Sequential Matching Framework for Multi-turn Response Selection in Retrieval-based Chatbots</p>
<p><a href="https://export.arxiv.org/pdf/1710.11344.pdf">https://export.arxiv.org/pdf/1710.11344.pdf</a></p>
<p><strong>å¯¹è¯è¡¨ç¤ºå­¦ä¹ -ç”¨äºæ£€ç´¢</strong></p>
<p><strong>dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings</strong></p>
<p>Reasoning With Neural Tensor Networks for Knowledge Base Completion</p>
<p><a href="https://papers.nips.cc/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf">https://papers.nips.cc/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf</a></p>
<p>Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots</p>
<p><a href="https://github.com/chunyuanY/Dialogue">https://github.com/chunyuanY/Dialogue</a></p>
<h2 id="32-codeç”Ÿæˆ">32 Codeç”Ÿæˆ<a hidden class="anchor" aria-hidden="true" href="#32-codeç”Ÿæˆ">#</a></h2>
<p>Repository-Level Prompt Generation for Large Language Models of Code</p>
<h2 id="33-sqlç”Ÿæˆ">33 SQLç”Ÿæˆ<a hidden class="anchor" aria-hidden="true" href="#33-sqlç”Ÿæˆ">#</a></h2>
<p>MIGA: A Unified Multi-task Generation Framework for Conversational Text-to-SQL</p>
<p><a href="https://arxiv.org/pdf/2212.09278.pdf">https://arxiv.org/pdf/2212.09278.pdf</a></p>
<h2 id="34-gpt-4">34 GPT-4<a hidden class="anchor" aria-hidden="true" href="#34-gpt-4">#</a></h2>
<p><a href="https://hub.baai.ac.cn/view/24839">https://hub.baai.ac.cn/view/24839</a></p>
<p>3æœˆ14æ—¥ï¼ŒOpen AIå®˜ç½‘å‘å¸ƒGPT-4ï¼Œæ”¯æŒå›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œæ•ˆæœè¶…è¶ŠChatGPTã€‚</p>
<p>GPT-4 å®ç°äº†é£è·ƒå¼æå‡ï¼šå¼ºå¤§çš„è¯†å›¾èƒ½åŠ›ï¼›æ–‡å­—è¾“å…¥é™åˆ¶æå‡è‡³ 2.5 ä¸‡å­—ï¼›å›ç­”å‡†ç¡®æ€§æ˜¾è‘—æé«˜ï¼›èƒ½å¤Ÿç”Ÿæˆæ­Œè¯ã€åˆ›æ„æ–‡æœ¬ï¼Œå®ç°é£æ ¼å˜åŒ–ã€‚</p>
<p>å®˜ç½‘åœ°å€ï¼š<em><a href="https://openai.com/product/gpt-4">https://openai.com/product/gpt-4</a></em></p>
<p>è®ºæ–‡ä¸‹è½½ï¼š</p>
<p><a href="https://event-cdn.baai.ac.cn/file/file-browser/BckxAwHQdMdCerFZpXDhhJba6JTWWTZd.pdf">https://event-cdn.baai.ac.cn/file/file-browser/BckxAwHQdMdCerFZpXDhhJba6JTWWTZd.pdf</a></p>
<p>ç›´æ’­åœ°å€ï¼š</p>
<p><a href="https://www.youtube.com/watch?v=outcGtbnMuQ">https://www.youtube.com/watch?v=outcGtbnMuQ</a></p>
<p>è´¡çŒ®è€…ï¼š
<a href="https://openai.com/contributions/gpt-4">https://openai.com/contributions/gpt-4</a></p>
<h2 id="35-chatglmåƒäº¿åŸºåº§çš„å¯¹è¯æ¨¡å‹å¼€å¯å†…æµ‹-å¯¹åº”å•å¡ç‰ˆæœ¬å¼€æº">35 ChatGLMï¼šåƒäº¿åŸºåº§çš„å¯¹è¯æ¨¡å‹å¼€å¯å†…æµ‹ â¸ºå¯¹åº”å•å¡ç‰ˆæœ¬å¼€æº<a hidden class="anchor" aria-hidden="true" href="#35-chatglmåƒäº¿åŸºåº§çš„å¯¹è¯æ¨¡å‹å¼€å¯å†…æµ‹-å¯¹åº”å•å¡ç‰ˆæœ¬å¼€æº">#</a></h2>
<p>ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº <a href="https://github.com/THUDM/GLM">General Language Model (GLM)</a> æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 6GB æ˜¾å­˜ï¼‰ã€‚ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ã€‚</p>
<p>ä»£ç é“¾æ¥ï¼šhttps://github.com/THUDM/ChatGLM-6B.git</p>
<p>å…·ä½“æ¥è¯´ï¼ŒChatGLM-6Bå…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š</p>
<ul>
<li>**å……åˆ†çš„ä¸­è‹±åŒè¯­é¢„è®­ç»ƒï¼š**ChatGLM-6Båœ¨1:1æ¯”ä¾‹çš„ä¸­è‹±è¯­æ–™ä¸Šè®­ç»ƒäº†1Tçš„tokené‡ï¼Œå…¼å…·åŒè¯­èƒ½åŠ›ã€‚</li>
<li>**ä¼˜åŒ–çš„æ¨¡å‹æ¶æ„å’Œå¤§å°ï¼š**å¸å–GLM-130Bè®­ç»ƒç»éªŒï¼Œä¿®æ­£äº†äºŒç»´RoPEä½ç½®ç¼–ç å®ç°ï¼Œä½¿ç”¨ä¼ ç»ŸFFNç»“æ„ã€‚6Bï¼ˆ62äº¿ï¼‰çš„å‚æ•°å¤§å°ï¼Œä¹Ÿä½¿å¾—ç ”ç©¶è€…å’Œä¸ªäººå¼€å‘è€…è‡ªå·±å¾®è°ƒå’Œéƒ¨ç½²ChatGLM-6Bæˆä¸ºå¯èƒ½ã€‚</li>
<li>**è¾ƒä½çš„éƒ¨ç½²é—¨æ§›ï¼š**FP16 åŠç²¾åº¦ä¸‹ï¼ŒChatGLM-6B éœ€è¦è‡³å°‘ 13 GB çš„æ˜¾å­˜è¿›è¡Œæ¨ç†ï¼Œç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œè¿™ä¸€éœ€æ±‚å¯ä»¥è¿›ä¸€æ­¥é™ä½åˆ° 10GBï¼ˆINT8ï¼‰ å’Œ 6GBï¼ˆINT4ï¼‰ï¼Œä½¿å¾— ChatGLM-6B å¯ä»¥éƒ¨ç½²åœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šã€‚</li>
<li>**æ›´é•¿çš„åºåˆ—é•¿åº¦ï¼š**ç›¸æ¯” GLM-10Bï¼ˆåºåˆ—é•¿åº¦1024ï¼‰ï¼ŒChatGLM-6Båºåˆ—é•¿åº¦è¾¾2048ï¼Œæ”¯æŒæ›´é•¿å¯¹è¯å’Œåº”ç”¨ã€‚</li>
<li>**äººç±»æ„å›¾å¯¹é½è®­ç»ƒï¼š**ä½¿ç”¨äº†ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰ã€åé¦ˆè‡ªåŠ©ï¼ˆFeedback Bootstrapï¼‰ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning from Human Feedbackï¼‰ç­‰æ–¹å¼ï¼Œä½¿æ¨¡å‹åˆå…·ç†è§£äººç±»æŒ‡ä»¤æ„å›¾çš„èƒ½åŠ›ã€‚è¾“å‡ºæ ¼å¼ä¸ºmarkdownï¼Œæ–¹ä¾¿å±•ç¤ºã€‚</li>
</ul>
<p><strong>ä»£ç è°ƒç”¨</strong></p>
<p>å¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM-6B æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> <span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer, AutoModel
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;THUDM/chatglm-6b&#34;</span>, trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> model <span style="color:#f92672">=</span> AutoModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;THUDM/chatglm-6b&#34;</span>, trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">.</span>half()<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> response, history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>chat(tokenizer, <span style="color:#e6db74">&#34;ä½ å¥½&#34;</span>, history<span style="color:#f92672">=</span>[])
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> print(response)
</span></span><span style="display:flex;"><span>ä½ å¥½<span style="color:#960050;background-color:#1e0010">ğŸ‘‹!</span>æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM<span style="color:#f92672">-</span><span style="color:#ae81ff">6</span>B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜<span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> response, history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>chat(tokenizer, <span style="color:#e6db74">&#34;æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ&#34;</span>, history<span style="color:#f92672">=</span>history)
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> print(response)
</span></span><span style="display:flex;"><span>æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1.</span> åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡<span style="color:#960050;background-color:#1e0010">ã€‚</span>å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠ<span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2.</span> åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œ<span style="color:#960050;background-color:#1e0010">ã€‚</span>å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£<span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3.</span> æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡<span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4.</span> é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡<span style="color:#960050;background-color:#1e0010">ã€‚</span>å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹<span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5.</span> é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ <span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">6.</span> å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡<span style="color:#960050;background-color:#1e0010">ã€‚</span>è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”<span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®<span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span></code></pre></div><h2 id="36-å¤šæ¨¡æ€å­¦ä¹ ">36 å¤šæ¨¡æ€å­¦ä¹ <a hidden class="anchor" aria-hidden="true" href="#36-å¤šæ¨¡æ€å­¦ä¹ ">#</a></h2>
<p><a href="https://zhuanlan.zhihu.com/p/582878508">https://zhuanlan.zhihu.com/p/582878508</a></p>
<p>å¤šæ¨¡æ€å¯¹è¯ Multimodal Dialogue Response Generation</p>
<p>10:59</p>
<p>å›¾åƒï¼‹å°æ¨¡å‹å®ç°æ€ç»´é“¾</p>
<pre tabindex="0"><code>è¿™ä¸€ç¯‡ï¼š
Multimodal Chain-of-Thought Reasoning in Language Models
</code></pre><pre tabindex="0"><code>å®ƒ å°±æ˜¯è®¾è®¡äº†2ä¸ªç‹¬ç«‹çš„ transformerï¼Œè¯­è¨€çš„å’Œ å›¾åƒçš„ï¼Œç„¶åæŠŠä»–ä»¬åœ¨ decoderçš„è¾“å…¥å±‚åšäº†concatï¼ŒåŸºäºä»»åŠ¡å†åšå¾®è°ƒã€‚
</code></pre><pre tabindex="0"><code>æ²¡å•¥æ–°æ„æ€ï¼Œæ€æƒ³è·Ÿ çŸ¥è¯†åº“é€‚é…å™¨ é‚£ä¸€ç¯‡æ˜¯ä¸€è‡´çš„ã€‚
K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. ï¼ˆå¤šçŸ¥è¯†æºæŒ‚è½½çš„ï¼‰
</code></pre><blockquote>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªä¸ºå¤šæ¨¡æ€è®¾è®¡çš„æ¦‚ç‡å»ºæ¨¡æ¡†æ¶ UniDiffuserï¼Œé™¤äº†å•å‘çš„æ–‡ç”Ÿå›¾ï¼Œè¿˜èƒ½å®ç°å›¾ç”Ÿæ–‡ã€å›¾æ–‡è”åˆç”Ÿæˆã€æ— æ¡ä»¶å›¾æ–‡ç”Ÿæˆã€å›¾æ–‡æ”¹å†™ç­‰å¤šç§åŠŸèƒ½ã€‚</p>
</blockquote>
<p>æ®æ‚‰ GPT-4 å°†äºæœ¬å‘¨å‘å¸ƒï¼Œå¤šæ¨¡æ€å°†æˆä¸ºå…¶ä¸€å¤§äº®ç‚¹ã€‚å½“å‰çš„å¤§è¯­è¨€æ¨¡å‹æ­£åœ¨æˆä¸ºç†è§£å„ç§æ¨¡æ€çš„é€šç”¨æ¥å£ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒæ¨¡æ€ä¿¡æ¯æ¥ç»™å‡ºå›å¤æ–‡æœ¬ï¼Œä½†å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ä¹Ÿä»…ä»…å±€é™äºæ–‡æœ¬ã€‚å¦ä¸€æ–¹é¢ï¼Œå½“å‰çš„æ‰©æ•£æ¨¡å‹ DALLãƒ»E 2ã€Imagenã€Stable Diffusion ç­‰åœ¨è§†è§‰åˆ›ä½œä¸Šæ€èµ·ä¸€åœºé©å‘½ï¼Œä½†è¿™äº›æ¨¡å‹ä»…ä»…æ”¯æŒæ–‡åˆ°å›¾çš„å•ä¸€è·¨æ¨¡æ€åŠŸèƒ½ï¼Œç¦»é€šç”¨å¼ç”Ÿæˆæ¨¡å‹è¿˜æœ‰ä¸€å®šè·ç¦»ã€‚è€Œå¤šæ¨¡æ€å¤§æ¨¡å‹å°†èƒ½å¤Ÿæ‰“é€šå„ç§æ¨¡æ€èƒ½åŠ›ï¼Œå®ç°ä»»æ„æ¨¡æ€ä¹‹é—´è½¬åŒ–ï¼Œè¢«è®¤ä¸ºæ˜¯é€šç”¨å¼ç”Ÿæˆæ¨¡å‹çš„æœªæ¥å‘å±•æ–¹å‘ã€‚</p>
<p>æ¸…åå¤§å­¦è®¡ç®—æœºç³»æœ±å†›æ•™æˆå¸¦é¢†çš„ TSAIL å›¢é˜Ÿè¿‘æœŸå…¬å¼€çš„ä¸€ç¯‡è®ºæ–‡ã€ŠOne Transformer Fits All Distributions in Multi-Modal Diffusion at Scaleã€‹ï¼Œç‡å…ˆå‘å¸ƒäº†å¯¹å¤šæ¨¡æ€ç”Ÿæˆå¼æ¨¡å‹çš„ä¸€äº›æ¢ç´¢å·¥ä½œï¼Œå®ç°äº†ä»»æ„æ¨¡æ€ä¹‹é—´çš„ç›¸äº’è½¬åŒ–ã€‚</p>
<p><img loading="lazy" src="0dd7912397dda1446b3469d529c60ba90cf48611.png" alt="img"  />
</p>
<p>è®ºæ–‡é“¾æ¥ï¼š
<a href="https://ml.cs.tsinghua.edu.cn/diffusion/unidiffuser.pdf">https://ml.cs.tsinghua.edu.cn/diffusion/unidiffuser.pdf</a></p>
<p>å¼€æºä»£ç ï¼š
<a href="https://github.com/thu-ml/unidiffuser">https://github.com/thu-ml/unidiffuser</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="http://example.org/">My New Hugo Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
